{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Learning Methodology\n",
    "<h4>Team Twin AI</h4>\n",
    "<h4><b>Overview</b></h4>\n",
    "\n",
    "This is the Machine Learning component of our solution to the FormulaAI Hack 2022 Competition. The workflow for this notebook is outlined as follows:\n",
    "- Standardisation and Pipelines\n",
    "- Model Experimentations I: Weather Classification\n",
    "- Leaderboard Ranking and Holdout Evaluation I\n",
    "- Model Experimentations II: Regression\n",
    "- Leaderboard Ranking and Holdout Evaluation II\n",
    "- Predictions and Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T12:53:14.708729Z",
     "iopub.status.busy": "2022-03-01T12:53:14.708051Z",
     "iopub.status.idle": "2022-03-01T12:53:14.724106Z",
     "shell.execute_reply": "2022-03-01T12:53:14.723151Z",
     "shell.execute_reply.started": "2022-03-01T12:53:14.708692Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None, 'display.max_rows', 100)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "#import deepchecks as dc\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import GenericUnivariateSelect\n",
    "from sklearn.preprocessing import scale,StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRFClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "#from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRFRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <i>Read the data<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T09:56:42.500007Z",
     "iopub.status.busy": "2022-03-01T09:56:42.499604Z",
     "iopub.status.idle": "2022-03-01T09:56:43.581628Z",
     "shell.execute_reply": "2022-03-01T09:56:43.580437Z",
     "shell.execute_reply.started": "2022-03-01T09:56:42.499959Z"
    }
   },
   "outputs": [],
   "source": [
    "final_data_weather = pd.read_csv('../input/final-weather-data-formulaai/final_data_weather.csv', index_col = False)\n",
    "print(final_data_weather.shape)\n",
    "final_data_weather.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T09:56:43.583081Z",
     "iopub.status.busy": "2022-03-01T09:56:43.582856Z",
     "iopub.status.idle": "2022-03-01T09:56:43.638987Z",
     "shell.execute_reply": "2022-03-01T09:56:43.637586Z",
     "shell.execute_reply.started": "2022-03-01T09:56:43.583048Z"
    }
   },
   "outputs": [],
   "source": [
    "weather_X = final_data_weather.drop('M_WEATHER', axis=1)\n",
    "weather_y = final_data_weather['M_WEATHER']\n",
    "\n",
    "rain_X = final_data_weather.drop('M_RAIN_PERCENTAGE', axis=1)\n",
    "rain_y = final_data_weather['M_RAIN_PERCENTAGE']\n",
    "\n",
    "print(weather_X.shape)\n",
    "print(rain_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h4><b>1. Cross-Validation, Standardisation and Pipelines</b></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Creating train, test and validation sets***\n",
    "    \n",
    "We first split our data into train and test sets. The test set is our holdout set and will not be unlocked until the end of each of the 2 sequences of experiments for classification and regression, respectively. The validation set will be split out of the train data and will be used for primary evaluation and to compute cross validation scores in each of our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T09:56:46.137229Z",
     "iopub.status.busy": "2022-03-01T09:56:46.136942Z",
     "iopub.status.idle": "2022-03-01T09:56:46.250599Z",
     "shell.execute_reply": "2022-03-01T09:56:46.249851Z",
     "shell.execute_reply.started": "2022-03-01T09:56:46.137178Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(weather_X, weather_y, test_size=.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the huge class imbalance in the weather target, ***we will implement repeated k-fold cross validation to further split our train data***. For our classification experiments, we will use ***repeated stratified k-fold cross validation***. We choose the value of 10 for *k* as this value has been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance. In other words, we are choosing *k = 10* to achieve reasonable bias-variance trade-off in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T09:56:47.9368Z",
     "iopub.status.busy": "2022-03-01T09:56:47.936325Z",
     "iopub.status.idle": "2022-03-01T09:56:47.940887Z",
     "shell.execute_reply": "2022-03-01T09:56:47.940052Z",
     "shell.execute_reply.started": "2022-03-01T09:56:47.936765Z"
    }
   },
   "outputs": [],
   "source": [
    "skfold = RepeatedStratifiedKFold(n_splits=10, n_repeats = 2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define a helper function that we will use for all our classification experiments. This function will also help to return validation scores for each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T10:03:37.673186Z",
     "iopub.status.busy": "2022-03-01T10:03:37.672514Z",
     "iopub.status.idle": "2022-03-01T10:03:37.681932Z",
     "shell.execute_reply": "2022-03-01T10:03:37.681007Z",
     "shell.execute_reply.started": "2022-03-01T10:03:37.67315Z"
    }
   },
   "outputs": [],
   "source": [
    "def training(X_train, y_train, model):\n",
    "\n",
    "    fold_no = 1    \n",
    "    n_scores, log_scores = [],[]\n",
    "    for train_index, val_index in skfold.split(X_train, y_train):\n",
    "        # select rows\n",
    "        train_X, val_X = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        train_y, val_y = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        #eval_set = [(val_X, val_y)]\n",
    "        model.fit(train_X, train_y)#, eval_metric=metric, eval_set=eval_set, verbose=False)\n",
    "        \n",
    "        n_scores.append(model.score(val_X, val_y))\n",
    "        log_scores.append(log_loss(val_y, model.predict_proba(val_X), labels = [0,1,2]))\n",
    "        print('For Fold {}, the Accuracy is {},'.format(str(fold_no), n_scores[fold_no - 1]), \n",
    "              'and the LogLoss is', log_scores[fold_no - 1])\n",
    "\n",
    "        fold_no += 1\n",
    "     \n",
    "    #cv_score = cross_val_score(model, train_X, train_y, scoring='accuracy', cv=skfold, n_jobs=-1, error_score='raise')\n",
    "    mean_accuracy, std_accuracy = np.mean(n_scores), np.std(n_scores)\n",
    "    mean_loss, std_loss = np.mean(log_scores), np.std(log_scores)\n",
    "    \n",
    "    print('\\n======================================')\n",
    "    \n",
    "    print('Mean Accuracy: %.3f (%.3f)' % (mean_accuracy, std_accuracy))\n",
    "    #print('Mean CV Accuracy: %.3f (%.3f)' % (np.mean(cv_score), np.std(cv_score)))\n",
    "    #\n",
    "    #print('\\n======================================')\n",
    "    #\n",
    "    print('Average LogLoss: {} ({})'.format(mean_loss, std_loss))\n",
    "\n",
    "    return model, mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***To avoid information leakage from our test data into the models we want to train, we will make use of pipelines in most of our experiments.***\n",
    "\n",
    "We define a pipeline construct below that implements standardisation on our data to make it Gaussian distributed, then fits a model on the standardised data. For experiments on just raw features, we will implement a pipeline without Standard Scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T10:49:46.21273Z",
     "iopub.status.busy": "2022-03-01T10:49:46.212279Z",
     "iopub.status.idle": "2022-03-01T10:49:46.217146Z",
     "shell.execute_reply": "2022-03-01T10:49:46.216482Z",
     "shell.execute_reply.started": "2022-03-01T10:49:46.212697Z"
    }
   },
   "outputs": [],
   "source": [
    "def scale_pipe(model_name, x_train, y_train):\n",
    "    \"\"\"\n",
    "    This function standardises the data to make it Gaussian distributed, then applies a \n",
    "    pipeline construct to fit a model on the standardised data\n",
    "    \"\"\"\n",
    "    trans = StandardScaler()\n",
    "    pipe = Pipeline([('scaler', trans), ('model', model_name)])\n",
    "    \n",
    "    model_pipeline, mean_loss = training(x_train, y_train, pipe)\n",
    "    \n",
    "    return model_pipeline, mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will experiment with various models with and without standardisation and see how they perform. But before we proceed, let's see what our data looks like when standardised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T01:52:13.732428Z",
     "iopub.status.busy": "2022-03-01T01:52:13.731701Z",
     "iopub.status.idle": "2022-03-01T01:52:13.966253Z",
     "shell.execute_reply": "2022-03-01T01:52:13.965446Z",
     "shell.execute_reply.started": "2022-03-01T01:52:13.732389Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "pd.DataFrame(scaler.fit_transform(final_data_weather), columns = final_data_weather.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h4><b>2. Model Experimentations I: Weather Classification</b></h4>\n",
    "Here are the 3 classification algorithms we will experiment with:\n",
    "\n",
    "- XGBoost Classifier\n",
    "- XGB Random Forest Classifier\n",
    "- Light Gradient Boosted Machines Classifier\n",
    "- Gradient Boosted Trees with CatBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(a) XGBoost Classifier**\n",
    "\n",
    "*First Experiment: Without Standardisation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T10:05:20.26992Z",
     "iopub.status.busy": "2022-03-01T10:05:20.269655Z",
     "iopub.status.idle": "2022-03-01T10:23:10.82529Z",
     "shell.execute_reply": "2022-03-01T10:23:10.824567Z",
     "shell.execute_reply.started": "2022-03-01T10:05:20.269891Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "model_xgb, model_xgb_loss = training(X_train, y_train, xgb)\n",
    "model_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T10:24:38.414225Z",
     "iopub.status.busy": "2022-03-01T10:24:38.41393Z",
     "iopub.status.idle": "2022-03-01T10:24:38.419418Z",
     "shell.execute_reply": "2022-03-01T10:24:38.418742Z",
     "shell.execute_reply.started": "2022-03-01T10:24:38.414177Z"
    }
   },
   "outputs": [],
   "source": [
    "model_xgb_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Second Experiment: With Standardisation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T10:49:57.466679Z",
     "iopub.status.busy": "2022-03-01T10:49:57.465885Z",
     "iopub.status.idle": "2022-03-01T11:07:45.884402Z",
     "shell.execute_reply": "2022-03-01T11:07:45.883594Z",
     "shell.execute_reply.started": "2022-03-01T10:49:57.466622Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb2 = XGBClassifier()\n",
    "xgb_scaled, xgb_scaled_loss = scale_pipe(xgb, X_train, y_train)\n",
    "xgb_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T11:07:45.886874Z",
     "iopub.status.busy": "2022-03-01T11:07:45.886372Z",
     "iopub.status.idle": "2022-03-01T11:07:45.893114Z",
     "shell.execute_reply": "2022-03-01T11:07:45.892363Z",
     "shell.execute_reply.started": "2022-03-01T11:07:45.886836Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_scaled_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(b) XGBoost Random Forest Classifier**\n",
    "\n",
    "*First Experiment: Without Standardisation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T11:11:25.877214Z",
     "iopub.status.busy": "2022-03-01T11:11:25.876925Z",
     "iopub.status.idle": "2022-03-01T11:19:55.626422Z",
     "shell.execute_reply": "2022-03-01T11:19:55.62586Z",
     "shell.execute_reply.started": "2022-03-01T11:11:25.877168Z"
    }
   },
   "outputs": [],
   "source": [
    "xgbrf = XGBRFClassifier(n_estimators=100, subsample=0.9, colsample_bynode=0.2)\n",
    "xgb_forest, xgb_forest_loss = training(X_train, y_train, xgbrf)\n",
    "xgb_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T11:19:55.628981Z",
     "iopub.status.busy": "2022-03-01T11:19:55.628289Z",
     "iopub.status.idle": "2022-03-01T11:19:55.635618Z",
     "shell.execute_reply": "2022-03-01T11:19:55.634917Z",
     "shell.execute_reply.started": "2022-03-01T11:19:55.628943Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_forest_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Second Experiment: With Standardisation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T11:21:26.14179Z",
     "iopub.status.busy": "2022-03-01T11:21:26.140324Z",
     "iopub.status.idle": "2022-03-01T11:30:03.119791Z",
     "shell.execute_reply": "2022-03-01T11:30:03.119095Z",
     "shell.execute_reply.started": "2022-03-01T11:21:26.141755Z"
    }
   },
   "outputs": [],
   "source": [
    "xgbrf2 = XGBRFClassifier(n_estimators=100, subsample=0.9, colsample_bynode=0.2)\n",
    "xgb_forest_scaled, xgb_forest_scaled_loss = scale_pipe(xgbrf2, X_train, y_train)\n",
    "xgb_forest_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T11:30:03.121998Z",
     "iopub.status.busy": "2022-03-01T11:30:03.121319Z",
     "iopub.status.idle": "2022-03-01T11:30:03.127839Z",
     "shell.execute_reply": "2022-03-01T11:30:03.126938Z",
     "shell.execute_reply.started": "2022-03-01T11:30:03.121962Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_forest_scaled_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(c) Light Gradient Boosted Machines Classifier**\n",
    "\n",
    "*First Experiment: Without Standardisation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T11:31:59.792763Z",
     "iopub.status.busy": "2022-03-01T11:31:59.792508Z",
     "iopub.status.idle": "2022-03-01T11:34:41.95538Z",
     "shell.execute_reply": "2022-03-01T11:34:41.954813Z",
     "shell.execute_reply.started": "2022-03-01T11:31:59.792733Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb = LGBMClassifier()\n",
    "lgb_model, lgb_loss = training(X_train, y_train, lgb)\n",
    "lgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T11:34:41.958706Z",
     "iopub.status.busy": "2022-03-01T11:34:41.958403Z",
     "iopub.status.idle": "2022-03-01T11:34:41.963979Z",
     "shell.execute_reply": "2022-03-01T11:34:41.963048Z",
     "shell.execute_reply.started": "2022-03-01T11:34:41.958676Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Second Experiment: With Standardisation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T11:34:41.965891Z",
     "iopub.status.busy": "2022-03-01T11:34:41.965628Z",
     "iopub.status.idle": "2022-03-01T11:37:20.27729Z",
     "shell.execute_reply": "2022-03-01T11:37:20.276696Z",
     "shell.execute_reply.started": "2022-03-01T11:34:41.965856Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb2 = LGBMClassifier()\n",
    "lgb_scaled, lgb_scaled_loss = scale_pipe(lgb2, X_train, y_train)\n",
    "lgb_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T11:37:20.280549Z",
     "iopub.status.busy": "2022-03-01T11:37:20.280233Z",
     "iopub.status.idle": "2022-03-01T11:37:20.286496Z",
     "shell.execute_reply": "2022-03-01T11:37:20.285481Z",
     "shell.execute_reply.started": "2022-03-01T11:37:20.280519Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb_scaled_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(d) Gradient Boosted Trees with CatBoost Classifier**\n",
    "\n",
    "*First Experiment: Without Standardisation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T11:38:07.85251Z",
     "iopub.status.busy": "2022-03-01T11:38:07.852256Z",
     "iopub.status.idle": "2022-03-01T11:42:52.640189Z",
     "shell.execute_reply": "2022-03-01T11:42:52.639465Z",
     "shell.execute_reply.started": "2022-03-01T11:38:07.852482Z"
    }
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "ctb = CatBoostClassifier(verbose=0, n_estimators=100)\n",
    "ctb_model, ctb_loss = training(X_train, y_train, ctb)\n",
    "ctb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T11:42:52.642487Z",
     "iopub.status.busy": "2022-03-01T11:42:52.641863Z",
     "iopub.status.idle": "2022-03-01T11:42:52.648117Z",
     "shell.execute_reply": "2022-03-01T11:42:52.647343Z",
     "shell.execute_reply.started": "2022-03-01T11:42:52.642447Z"
    }
   },
   "outputs": [],
   "source": [
    "ctb_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Second Experiment: With Standardisation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T11:46:06.893657Z",
     "iopub.status.busy": "2022-03-01T11:46:06.893166Z",
     "iopub.status.idle": "2022-03-01T11:50:54.003132Z",
     "shell.execute_reply": "2022-03-01T11:50:54.002451Z",
     "shell.execute_reply.started": "2022-03-01T11:46:06.89362Z"
    }
   },
   "outputs": [],
   "source": [
    "ctb2 = CatBoostClassifier(verbose=0, n_estimators=100)\n",
    "ctb_scaled, ctb_scaledloss = scale_pipe(ctb2, X_train, y_train)\n",
    "ctb_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T11:50:54.004914Z",
     "iopub.status.busy": "2022-03-01T11:50:54.004669Z",
     "iopub.status.idle": "2022-03-01T11:50:54.01134Z",
     "shell.execute_reply": "2022-03-01T11:50:54.010677Z",
     "shell.execute_reply.started": "2022-03-01T11:50:54.004882Z"
    }
   },
   "outputs": [],
   "source": [
    "ctb_scaledloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h4><b>3. Leaderboard Ranking and Holdout Evaluation I </b></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we unleash our holdout set which is 25% of our entire dataset, we will create a leaderboard of models built so far, to rank them by logloss both with standardisation and without it. This will give us an insight whether to evaluate on a transform of our test set or to test the raw features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T12:10:03.75333Z",
     "iopub.status.busy": "2022-03-01T12:10:03.753058Z",
     "iopub.status.idle": "2022-03-01T12:10:03.766083Z",
     "shell.execute_reply": "2022-03-01T12:10:03.765259Z",
     "shell.execute_reply.started": "2022-03-01T12:10:03.753299Z"
    }
   },
   "outputs": [],
   "source": [
    "unpiped_models = [model_xgb_loss, xgb_forest_loss, lgb_loss, ctb_loss]\n",
    "piped_models = [xgb_scaled_loss, xgb_forest_scaled_loss, lgb_scaled_loss, ctb_scaledloss]\n",
    "model_index = ['XGBoost','XGBoost Random Forest','LightGBM', 'CatBoost Classifier']\n",
    "\n",
    "pre_leaderboard_1 = pd.DataFrame({'Log Loss (Without Standardisation)': unpiped_models, \n",
    "                                  'Log Loss (With Standardisation)': piped_models}, index = model_index)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.10f' % x)\n",
    "pre_leaderboard_1.round(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's set the tabe in ascending order, starting with the first column. We are ranking in ascending order as the goal of the learning algorithms is to minimise the log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T12:11:47.975891Z",
     "iopub.status.busy": "2022-03-01T12:11:47.975232Z",
     "iopub.status.idle": "2022-03-01T12:11:47.986007Z",
     "shell.execute_reply": "2022-03-01T12:11:47.984335Z",
     "shell.execute_reply.started": "2022-03-01T12:11:47.975854Z"
    }
   },
   "outputs": [],
   "source": [
    "pre_leaderboard_1.nsmallest(4, 'Log Loss (Without Standardisation)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T12:11:54.012942Z",
     "iopub.status.busy": "2022-03-01T12:11:54.012692Z",
     "iopub.status.idle": "2022-03-01T12:11:54.022891Z",
     "shell.execute_reply": "2022-03-01T12:11:54.022083Z",
     "shell.execute_reply.started": "2022-03-01T12:11:54.012912Z"
    }
   },
   "outputs": [],
   "source": [
    "pre_leaderboard_1.nsmallest(4, 'Log Loss (With Standardisation)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, we can see that only XGBoost Random Forest performed better with standardisation. In both cases, XGBoost outperformed other models by a large margin. \n",
    "\n",
    "***We will therefore select XGBoost and unlock our test (holdout) set for evaluation without standardisation.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T12:45:55.788872Z",
     "iopub.status.busy": "2022-03-01T12:45:55.788604Z",
     "iopub.status.idle": "2022-03-01T12:46:55.508631Z",
     "shell.execute_reply": "2022-03-01T12:46:55.50787Z",
     "shell.execute_reply.started": "2022-03-01T12:45:55.78884Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb1 = model_xgb\n",
    "\n",
    "print('Average LogLoss from Cross Validation:', model_xgb_loss)\n",
    "print('\\n======================================')\n",
    "\n",
    "eval_set = [(X_test, y_test)]\n",
    "xgb1.fit(X_train, y_train, eval_metric = 'mlogloss', eval_set = eval_set, verbose=False)\n",
    "xgb1_eval = log_loss(y_test, xgb1.predict_proba(X_test), labels = [0,1,2])\n",
    "\n",
    "print('\\n======================================')\n",
    "print('LogLoss on Holdout:', xgb1_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h4><b>4. Model Experimentations I: Regression</b></h4>\n",
    "Here are the 4 regression algorithms we will experiment with:\n",
    "\n",
    "- RandomForest Regressor\n",
    "- Light Gradient Boosted Trees Regressor (with Early Stopping)\n",
    "- ElasticNet Regularised Regression with Grid Search Optimisation\n",
    "- XGBoost Regressor\n",
    "\n",
    "Before we proceed, we specify a new train-test split using the rain percentage data. We also change our k-fold algorithm to RepeatedKFold. In each of these experiments, we will use cross validation to give us insights into which model performs the best before we then unlock holdout to apply the best algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T13:12:13.800615Z",
     "iopub.status.busy": "2022-03-01T13:12:13.800359Z",
     "iopub.status.idle": "2022-03-01T13:12:13.904321Z",
     "shell.execute_reply": "2022-03-01T13:12:13.903567Z",
     "shell.execute_reply.started": "2022-03-01T13:12:13.800587Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(rain_X, rain_y, test_size=.25, random_state=42)\n",
    "kfold = RepeatedKFold(n_splits=10, n_repeats = 2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T10:03:37.673186Z",
     "iopub.status.busy": "2022-03-01T10:03:37.672514Z",
     "iopub.status.idle": "2022-03-01T10:03:37.681932Z",
     "shell.execute_reply": "2022-03-01T10:03:37.681007Z",
     "shell.execute_reply.started": "2022-03-01T10:03:37.67315Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_reg(X_train, y_train, model):\n",
    "\n",
    "    fold_no = 1    \n",
    "    n_scores, mae_scores = [],[]\n",
    "    for train_index, val_index in skfold.split(X_train, y_train):\n",
    "        # select rows\n",
    "        train_X, val_X = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        train_y, val_y = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        #eval_set = [(val_X, val_y)]\n",
    "        model.fit(train_X, train_y)#, eval_metric=metric, eval_set=eval_set, verbose=False)\n",
    "        \n",
    "        n_scores.append(model.score(val_X, val_y))\n",
    "        mae_scores.append(mean_absolute_error(val_y, model.predict(val_X)))\n",
    "        print('For Fold {}, the Accuracy is {},'.format(str(fold_no), n_scores[fold_no - 1]), \n",
    "              'and the LogLoss is', mae_scores[fold_no - 1])\n",
    "\n",
    "        fold_no += 1\n",
    "     \n",
    "    #cv_score = cross_val_score(model, train_X, train_y, scoring='accuracy', cv=skfold, n_jobs=-1, error_score='raise')\n",
    "    mean_accuracy, std_accuracy = np.mean(n_scores), np.std(n_scores)\n",
    "    mean_mae, std_loss = np.mean(mae_scores), np.std(mae_scores)\n",
    "    \n",
    "    print('\\n======================================')\n",
    "    \n",
    "    print('Mean Accuracy: %.3f (%.3f)' % (mean_accuracy, std_accuracy))\n",
    "    #print('Mean CV Accuracy: %.3f (%.3f)' % (np.mean(cv_score), np.std(cv_score)))\n",
    "    #\n",
    "    #print('\\n======================================')\n",
    "    #\n",
    "    print('Average MAE: {} ({})'.format(mean_mae, std_mae))\n",
    "\n",
    "    return model, mean_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(a) Random Forest Regressor**\n",
    "\n",
    "*First Experiment: Without Standardisation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-03-01T13:14:37.04761Z",
     "iopub.status.busy": "2022-03-01T13:14:37.047049Z",
     "iopub.status.idle": "2022-03-01T14:00:50.073079Z",
     "shell.execute_reply": "2022-03-01T14:00:50.07202Z",
     "shell.execute_reply.started": "2022-03-01T13:14:37.04757Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rforest = RandomForestRegressor(n_estimators=100)\n",
    "cv_scores = cross_val_score(rforest, X_train, y_train, scoring='neg_mean_absolute_error', \n",
    "                            cv=kfold, n_jobs=-1, error_score='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T14:02:06.242401Z",
     "iopub.status.busy": "2022-03-01T14:02:06.241858Z",
     "iopub.status.idle": "2022-03-01T14:02:06.247473Z",
     "shell.execute_reply": "2022-03-01T14:02:06.246637Z",
     "shell.execute_reply.started": "2022-03-01T14:02:06.242361Z"
    }
   },
   "outputs": [],
   "source": [
    "rforest_mae = np.mean(cv_scores)\n",
    "print('MAE and Standard Deviation: %.3f (%.3f)' % (rforest_mae, np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Second Experiment: With Standardisation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T14:20:06.222018Z",
     "iopub.status.busy": "2022-03-01T14:20:06.221763Z",
     "iopub.status.idle": "2022-03-01T14:20:06.228551Z",
     "shell.execute_reply": "2022-03-01T14:20:06.226939Z",
     "shell.execute_reply.started": "2022-03-01T14:20:06.221989Z"
    }
   },
   "outputs": [],
   "source": [
    "def reg_pipe(model_name, x_train, y_train):\n",
    "    \"\"\"\n",
    "    This function standardises the data to make it Gaussian distributed, \n",
    "    then applies reeated 10-fold cross-validation\n",
    "    \"\"\"\n",
    "    trans = StandardScaler()\n",
    "    pipe = Pipeline([('scaler', trans), ('cv', model_name)])\n",
    "    \n",
    "    scores = cross_val_score(pipe, x_train, y_train, scoring='neg_mean_absolute_error', \n",
    "                            cv=kfold, n_jobs=-1, error_score='raise')\n",
    "    scaled_mae = np.mean(scores)\n",
    "    \n",
    "    return pipe, scaled_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T14:20:13.316845Z",
     "iopub.status.busy": "2022-03-01T14:20:13.316572Z",
     "iopub.status.idle": "2022-03-01T14:29:29.49413Z",
     "shell.execute_reply": "2022-03-01T14:29:29.493446Z",
     "shell.execute_reply.started": "2022-03-01T14:20:13.316816Z"
    }
   },
   "outputs": [],
   "source": [
    "r_forest_scaled, r_forest_scaled_mae = reg_pipe(RandomForestRegressor(n_estimators = 20), X_train, y_train)\n",
    "r_forest_scaled_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(b) Light Gradient Boosted Trees Regressor (with Early Stopping)**\n",
    "\n",
    "*First Experiment: Without Standardisation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T14:48:25.430647Z",
     "iopub.status.busy": "2022-03-01T14:48:25.429927Z",
     "iopub.status.idle": "2022-03-01T14:49:25.715613Z",
     "shell.execute_reply": "2022-03-01T14:49:25.71506Z",
     "shell.execute_reply.started": "2022-03-01T14:48:25.430593Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb = LGBMRegressor()\n",
    "\n",
    "fold_no = 1    \n",
    "n_scores, mae_scores = [],[]\n",
    "for train_index, val_index in kfold.split(X_train, y_train):\n",
    "    # select rows\n",
    "    train_X, val_X = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    train_y, val_y = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    eval_set = [(val_X, val_y)]\n",
    "    lgb.fit(train_X, train_y, early_stopping_rounds=10, \n",
    "            eval_metric='mean_absolute_error', eval_set=eval_set, verbose=False)\n",
    "\n",
    "    n_scores.append(lgb.score(val_X, val_y))\n",
    "    mae_scores.append(mean_absolute_error(val_y, lgb.predict(val_X)))\n",
    "    print('For Fold {}, the Accuracy is {},'.format(str(fold_no), n_scores[fold_no - 1]), \n",
    "    'and the MAE is', mae_scores[fold_no - 1])\n",
    "\n",
    "    fold_no += 1\n",
    "                      \n",
    "lgb_mae = np.mean(mae_scores)\n",
    "\n",
    "print('\\n======================================')\n",
    "print('MAE and Standard Deviation: %.3f (%.3f)' % (lgb_mae, np.std(mae_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Second Experiment: With Standardisation (and Without Early Stopping)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T15:00:51.920691Z",
     "iopub.status.busy": "2022-03-01T15:00:51.920414Z",
     "iopub.status.idle": "2022-03-01T15:01:47.426701Z",
     "shell.execute_reply": "2022-03-01T15:01:47.425956Z",
     "shell.execute_reply.started": "2022-03-01T15:00:51.920662Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb_scaled, lgb_scaled_mae = reg_pipe(LGBMRegressor(), X_train, y_train)\n",
    "lgb_scaled_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(c) ElasticNet Regularised Regression with GridSearch Optimisation**\n",
    "\n",
    "*First Experiment: Without Standardisation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-03-01T15:46:46.995637Z",
     "iopub.status.busy": "2022-03-01T15:46:46.995382Z",
     "iopub.status.idle": "2022-03-01T19:25:14.698836Z",
     "shell.execute_reply": "2022-03-01T19:25:14.697543Z",
     "shell.execute_reply.started": "2022-03-01T15:46:46.995608Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "start_model = ElasticNet()\n",
    "\n",
    "# define grid\n",
    "grid = dict()\n",
    "grid['alpha'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]\n",
    "grid['l1_ratio'] = np.arange(0, 1, 0.01)\n",
    "\n",
    "# define search\n",
    "search = GridSearchCV(start_model, grid, scoring='neg_mean_absolute_error', cv=kfold, n_jobs=-1)\n",
    "\n",
    "# perform the search\n",
    "results = search.fit(X_train, y_train)\n",
    "\n",
    "# summarize\n",
    "print('MAE: %.3f' % results.best_score_)\n",
    "print('Config: %s' % results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement cross-validation for ElasticNet across train data\n",
    "\n",
    "enet = ElasticNet(alpha = results.list(results.best_params_.values())[0],\n",
    "                        l1_ratio = results.list(results.best_params_.values())[1])\n",
    "\n",
    "fold_no = 1    \n",
    "n_scores, mae_scores = [],[]\n",
    "for train_index, val_index in kfold.split(X_train, y_train):\n",
    "    # select rows\n",
    "    train_X, val_X = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    train_y, val_y = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    enet.fit(train_X, train_y)\n",
    "\n",
    "    n_scores.append(enet.score(val_X, val_y))\n",
    "    mae_scores.append(mean_absolute_error(val_y, enet.predict(val_X)))\n",
    "    print('For Fold {}, the Accuracy is {},'.format(str(fold_no), n_scores[fold_no - 1]), \n",
    "    'and the MAE is', mae_scores[fold_no - 1])\n",
    "\n",
    "    fold_no += 1\n",
    "                      \n",
    "enet_mae = np.mean(mae_scores)\n",
    "\n",
    "print('\\n======================================')\n",
    "print('MAE and Standard Deviation: %.3f (%.3f)' % (enet_mae, np.std(mae_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Second Experiment: With Standardisation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_scaled = ElasticNet(alpha = results.list(results.best_params_.values())[0],\n",
    "                        l1_ratio = results.list(results.best_params_.values())[1])\n",
    "\n",
    "enet_scaled, enet_scaled_mae = reg_pipe(enet_scaled, X_train, y_train)\n",
    "enet_scaled_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(d) XGBoost Regressor**\n",
    "\n",
    "*First Experiment: Without Standardisation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_reg = XGBRegressor(objective='reg:squarederror')\n",
    "cv_scores = cross_val_score(xgb_reg, X_train, y_train, scoring='neg_mean_absolute_error', \n",
    "                            cv=kfold, n_jobs=-1, error_score='raise')\n",
    "\n",
    "xgbr_mae = np.mean(cv_scores)\n",
    "print('MAE and Standard Deviation: %.3f (%.3f)' % (xgbr_mae, np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Second Experiment: With Standardisation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-03-01T11:30:27.661843Z",
     "iopub.status.busy": "2022-03-01T11:30:27.661328Z",
     "iopub.status.idle": "2022-03-01T11:31:12.341161Z",
     "shell.execute_reply": "2022-03-01T11:31:12.340035Z",
     "shell.execute_reply.started": "2022-03-01T11:30:27.661807Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "xgbr = XGBRegressor(objective='reg:squarederror')\n",
    "xgbr_scaled, xgb_scaled_mae = reg_pip(xgbr, X_train, y_train)\n",
    "xgb_scaled_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h4><b>5. Leaderboard Ranking and Holdout Evaluation II</b></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpiped_models_reg = [rforest_mae, lgb_mae, enet_mae, xgbr_mae]\n",
    "piped_models_reg = [scaled_rforest_mae, lgb_scaled_mae, enet_scaled_mae, xgbr_scaled_mae]\n",
    "reg_model_index = ['XGBoost','XGBoost Random Forest','LightGBM', 'CatBoost Classifier']\n",
    "\n",
    "pre_leaderboard_2 = pd.DataFrame({'MAE (Without Standardisation)': unpiped_models_reg, \n",
    "                                  'MAE (With Standardisation)': piped_models_reg}, index = reg_model_index)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.10f' % x)\n",
    "pre_leaderboard_2.round(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_leaderboard_1.nsmallest(4, 'MAE (Without Standardisation)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_leaderboard_1.nsmallest(4, 'MAE (With Standardisation)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Predictions and Exporting\n",
    "<h4> </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(X, classifier, regressor):\n",
    "    \"\"\"\n",
    "    returns a test json response of predictions across the time interval \n",
    "    of {5,10,15,30,60} in minutes after a session timestamp\n",
    "    \"\"\"\n",
    "    export = dict()\n",
    "    intervals = [5,10,15,30,60]\n",
    "    \n",
    "    for time in intervals:\n",
    "        #Assign an offset to the picked offset\n",
    "        X['M_TIME_OFFSET'] = time\n",
    "        weather_prediction = classifier.predict([X])\n",
    "        rain_prediction = regressor.predict([X])\n",
    "        \n",
    "        output[time] = {\n",
    "            'weather_type': int(weather_prediction[0][0]),\n",
    "            'rain_percentage' : round(rain_prediction[0][1],2)\n",
    "        }\n",
    "    return json.dumps(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feat = '../input/final-weather-data-formulaai/twin_ai_features.pkl'\n",
    "pickle.dump(train_df_fu11.columns, open(mode1_feat, 'wb'))\n",
    "\n",
    "\n",
    "model_file = '../input/final-weather-data-formulaai/twin_ai_model.h5'\n",
    "pickle.dump(moc_model, open('model_file.pkl', 'wb'))\n",
    "\n",
    "#object_1 = 1\n",
    "#object_2 = \"A string\"\n",
    "#object_3 = 5\n",
    "#\n",
    "#sample_list = [object_1, object_2, object_3]\n",
    "#file_name = \"sample.pkl\"\n",
    "#\n",
    "#open_file = open(file_name, \"wb\")\n",
    "#pickle.dump(sample_list, open_file)\n",
    "#open_file.close()\n",
    "#\n",
    "#open_file = open(file_name, \"rb\")\n",
    "#loaded_list = pickle.load(open_file)\n",
    "#open_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
