{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Learning to Hack Weather Conditions**\n#### A Machine Learning Workflow for Weather Forecasting\n#### Team Twin AI ####\n\n***Major Contributors to this Notebook:*** Zion Pibowei, Temitayo Adejuyigbe, Anosike Chimaobi Nice*","metadata":{}},{"cell_type":"markdown","source":"## Background","metadata":{}},{"cell_type":"markdown","source":"Formula 1 is one of the most competitive sports in the world. Engineers and technicians from every team use weather radar screens, provided by Ubimet to the teams, which allows them to track the current weather and make predictions during the race. Race engineers relay precise information to drivers, including:\n\n- How many minutes until it starts raining\n- Intensity of the rain\n- Which corner will be hit first by the rain\n- Duration of the rain\n\nPoints, and even races sometimes, are won and lost based on making sense of what the weather is going to do during a race, and being prepared as a team to act accordingly.\n\nTherefore, weather forecasting takes a big part on the possible outcome of a race.\n\nSimilarly, F1 2021, the official Formula 1 videogame developed by Codemasters, uses a physics engine that behaves like the real world.","metadata":{}},{"cell_type":"markdown","source":"## The Challenge\n\nIn this challenge, we will analyse historical weather data from the RedBull Racing eSports team to build a high-performing model that is able to make accurate weather predictions/forecasts. Our objective is to predict the weather type 5, 10, 15, 30 and 60 minutes after a timestamp, and the rain percentage probability at that time. \n\n***Our solution is divided into 4 parts, each constituting a workflow on its own:***\n\n- Part I: Initial Data Analysis and Preprocessing\n- Part II: EDA and Feature Selection\n- Part III: Modelling Methodology\n- Part IV: Predictions and Exporting","metadata":{}},{"cell_type":"markdown","source":"## Part I: Initial Data Analysis and Preprocessing\n<h4><b>Overview</b></h4>\n\nThis is the IDA and Preprocessing component of our solution to the FormulaAI Hack 2022 Competition. The workflow for this notebook is outlined as follows: \n- Getting the Data\n- First Insights: Making Sense of the Data\n- Data Integrity Assessments\n- Cleaning the Data\n","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:00:12.528716Z","iopub.execute_input":"2022-03-01T21:00:12.52902Z","iopub.status.idle":"2022-03-01T21:00:12.536057Z","shell.execute_reply.started":"2022-03-01T21:00:12.528991Z","shell.execute_reply":"2022-03-01T21:00:12.535013Z"}}},{"cell_type":"code","source":"import os\n\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', None, 'display.max_rows', 100)\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import figure\n%matplotlib inline\n\nimport seaborn as sns\nsns.set_context('notebook')\nsns.set_style('whitegrid')\nsns.set_palette('Blues_r')\n\n#!conda install deepchecks\nimport deepchecks as dc\nfrom deepchecks.checks.integrity.is_single_value import IsSingleValue\nfrom deepchecks.checks.integrity.data_duplicates import DataDuplicates\nfrom deepchecks.checks import DataDuplicates\nfrom deepchecks.checks.integrity import LabelAmbiguity\nfrom deepchecks.base import Dataset, Suite\n\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\n\nimport random\nimport time\nfrom datetime import datetime\n\nimport warnings\n# warnings.filterwarnings('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:01:32.992856Z","iopub.execute_input":"2022-03-01T22:01:32.993672Z","iopub.status.idle":"2022-03-01T22:01:37.846873Z","shell.execute_reply.started":"2022-03-01T22:01:32.993629Z","shell.execute_reply":"2022-03-01T22:01:37.846117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!conda install deepchecks -y","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:33:03.318697Z","iopub.execute_input":"2022-03-01T21:33:03.31931Z","iopub.status.idle":"2022-03-01T21:35:25.347421Z","shell.execute_reply.started":"2022-03-01T21:33:03.319268Z","shell.execute_reply":"2022-03-01T21:35:25.346603Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:01:44.805735Z","iopub.execute_input":"2022-03-01T22:01:44.806013Z","iopub.status.idle":"2022-03-01T22:01:44.818232Z","shell.execute_reply.started":"2022-03-01T22:01:44.805981Z","shell.execute_reply":"2022-03-01T22:01:44.817165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading the CSV file\ndata = pd.read_csv('../input/formulaaihackathon2022/weather.csv',low_memory=False)\n#data = pd.read_csv('weather.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:01:46.143076Z","iopub.execute_input":"2022-03-01T22:01:46.143596Z","iopub.status.idle":"2022-03-01T22:02:14.406042Z","shell.execute_reply.started":"2022-03-01T22:01:46.143546Z","shell.execute_reply":"2022-03-01T22:02:14.404956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<h4><b>1. First Insights: Making Sense of the Data </b></h4>","metadata":{}},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:02:14.412141Z","iopub.execute_input":"2022-03-01T22:02:14.414567Z","iopub.status.idle":"2022-03-01T22:02:14.427514Z","shell.execute_reply.started":"2022-03-01T22:02:14.414526Z","shell.execute_reply":"2022-03-01T22:02:14.426521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info() #check the overall information about the dataset","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-01T22:02:14.428906Z","iopub.execute_input":"2022-03-01T22:02:14.429198Z","iopub.status.idle":"2022-03-01T22:02:14.456973Z","shell.execute_reply.started":"2022-03-01T22:02:14.429161Z","shell.execute_reply":"2022-03-01T22:02:14.45615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:02:14.460348Z","iopub.execute_input":"2022-03-01T22:02:14.460567Z","iopub.status.idle":"2022-03-01T22:02:14.496701Z","shell.execute_reply.started":"2022-03-01T22:02:14.460538Z","shell.execute_reply":"2022-03-01T22:02:14.495943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>In this dataframe, we see that the last column is an unnamed column. Before we drop it, let us inspect the proportion of NaN values it contains.<br>","metadata":{}},{"cell_type":"code","source":"print('Missing values:',data['Unnamed: 58'].isnull().sum())\nprint('Proportion of missing values: {}%'.format(data['Unnamed: 58'].isnull().sum()/data.shape[0]*100))","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:02:14.498751Z","iopub.execute_input":"2022-03-01T22:02:14.499219Z","iopub.status.idle":"2022-03-01T22:02:14.520465Z","shell.execute_reply.started":"2022-03-01T22:02:14.499181Z","shell.execute_reply":"2022-03-01T22:02:14.519628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clearly, the unnamed column is entirely filled with missing values and, as such, has no impact in our workflow. It's presence in the data is most likely due to encoding. Thus, the first treatment to our data is to drop this column.","metadata":{}},{"cell_type":"code","source":"data.drop('Unnamed: 58', axis = 1, inplace = True)\ndata.shape[1]","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:02:14.521823Z","iopub.execute_input":"2022-03-01T22:02:14.522157Z","iopub.status.idle":"2022-03-01T22:02:14.983644Z","shell.execute_reply.started":"2022-03-01T22:02:14.522121Z","shell.execute_reply":"2022-03-01T22:02:14.982879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's obtain summary statistics for our data ","metadata":{}},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:02:14.98484Z","iopub.execute_input":"2022-03-01T22:02:14.985511Z","iopub.status.idle":"2022-03-01T22:02:18.811834Z","shell.execute_reply.started":"2022-03-01T22:02:14.985465Z","shell.execute_reply":"2022-03-01T22:02:18.810971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br><b>At a glance:</b>\n- A quick inspection of the count row shows us that a number of columns contain missing values, ranging from small to large.\n- A quick inspection of the standard deviation shows us that some columns have zero variance, indicating that <b>each of these columns contain ONLY ONE distinct value</b>. Typically, variables whose standard deviations tend to zero have fewer distinct values.\n- A quick inspection of the min and max shows us that the very columns having 0 standard deviation <b>contain equal values of minimum and maximum</b>, validating our claim that these columns have only one distinct value.\n\nIn this project, we will carry out exhaustive analysis of the data to address the implictations of the forgoing discoveries.</b>\n","metadata":{}},{"cell_type":"markdown","source":"<h4><b>2. Data Integrity Assessments</b></h4>\n<p>In this section, we will investigate the integrity of the data and uncover any data quality issues that may be present. The insights we obtain in this section will guide us on how to resolve these issues pragmatically in the next section.</p>","metadata":{}},{"cell_type":"markdown","source":"<p><b>(a) Unique Values</b></p>\n\nOur ultimate goal is to build <b>a model that learns the evolution of weather conditions over time</b>. Therefore, we are interested in columns that show variation of values over time. Columns that contain only one unique value <b>may not provide predictive power for the model</b>. We will validate this assumption when we implement feature contribution checks ahead of our model methodology.\n<p> First, we make a general inspection of the number of unique values contained in all the columns.</p>","metadata":{}},{"cell_type":"code","source":"data.nunique(axis=0).sort_values().to_frame() #check for unique values and sort them into frames","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-01T22:02:18.813516Z","iopub.execute_input":"2022-03-01T22:02:18.813788Z","iopub.status.idle":"2022-03-01T22:02:20.170767Z","shell.execute_reply.started":"2022-03-01T22:02:18.813751Z","shell.execute_reply":"2022-03-01T22:02:20.169995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above result, we can see that there are 7 columns that contain only one unique value. Below, we obtain further information about what these exact values are.","metadata":{}},{"cell_type":"code","source":"sv = IsSingleValue()\nsv.run(data)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T21:59:23.733041Z","iopub.execute_input":"2022-03-01T21:59:23.7336Z","iopub.status.idle":"2022-03-01T21:59:25.631123Z","shell.execute_reply.started":"2022-03-01T21:59:23.733561Z","shell.execute_reply":"2022-03-01T21:59:25.63047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<p><b>(b) Data Duplicates<b></p>\nWe need to run a duplicate check to find if there are multiple instances of identical samples in our dataset. One reason is that duplicates could be an indicator for a problem in the data pipeline that requires attention. The other is that they can potentially increase the weight that a machine learning model gives to samples. ","metadata":{}},{"cell_type":"code","source":"print('Proportion of duplicates: {}%'.format(len(data[data.duplicated()])/data.shape[0]*100))\ndata[data.duplicated()] #check for average duplicate values in the dataset","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-01T22:02:24.34477Z","iopub.execute_input":"2022-03-01T22:02:24.345077Z","iopub.status.idle":"2022-03-01T22:02:31.426708Z","shell.execute_reply.started":"2022-03-01T22:02:24.345042Z","shell.execute_reply":"2022-03-01T22:02:31.426002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, this is only partially informative. We only know that 2057230 samples, representing ~57% of the data, are duplicated. But this doesn't tell us the number of times each example of duplicate data appears. We will obtain the desired information by implementing the following additional checks.","metadata":{}},{"cell_type":"code","source":"#from deepchecks.checks import DataDuplicates\nDataDuplicates().run(data)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:02:42.254142Z","iopub.execute_input":"2022-03-01T22:02:42.254439Z","iopub.status.idle":"2022-03-01T22:08:06.114179Z","shell.execute_reply.started":"2022-03-01T22:02:42.254406Z","shell.execute_reply":"2022-03-01T22:08:06.113452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can summarise this check by defining a check condition that sets the baseline of duplicate ratio as 0. This will expose any violation to the condition and reveal the present duplicate ratio.","metadata":{}},{"cell_type":"code","source":"check = DataDuplicates()\ncheck.add_condition_ratio_not_greater_than(0)\nresult = check.run(data)\nresult.show(show_additional_outputs=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are interested in knowing whether the duplicates observed here were intentionally intended to be part of the data. However, if this is an hidden issue weâ€™re not expecting to occur, then we will need to resolve it. We will revisit this in the EDA component of our workflow.","metadata":{}},{"cell_type":"markdown","source":"<br>\n<p><b>(c) Label Ambiguity</b></p>","metadata":{}},{"cell_type":"markdown","source":"We would also like to check whether there are identical samples in the data with different labels. This alerts us to further verify whether or not the data was mislabelled, as mislabelled data could confuse the model and lead to lower model performance.","metadata":{}},{"cell_type":"code","source":"label_ambig = Dataset(data, label='M_WEATHER')\nLabelAmbiguity().run(label_ambig)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, we summarise this check by defining a check condition that sets the baseline of ambiguous sample ratio as 0. This will expose any violation to the condition and reveal the present ambiguous sample ratio.","metadata":{}},{"cell_type":"code","source":"check = LabelAmbiguity()\ncheck.add_condition_ambiguous_sample_ratio_not_greater_than(0)\nresult = check.run(label_ambig)\nresult.show(show_additional_outputs=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Indeed, we observe that there are no identical samples with different labels.","metadata":{}},{"cell_type":"markdown","source":"<br>\n<p><b>(d) Missing Values</b></p>","metadata":{}},{"cell_type":"code","source":"data.isna().sum().sort_values().to_frame() #check for missing values and sort them into frames","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:11:42.441213Z","iopub.execute_input":"2022-03-01T22:11:42.44192Z","iopub.status.idle":"2022-03-01T22:11:43.007952Z","shell.execute_reply.started":"2022-03-01T22:11:42.441881Z","shell.execute_reply":"2022-03-01T22:11:43.007228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.notna().sum().sort_values().to_frame() #check for non-missing values and sort them into frames","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:12:03.824584Z","iopub.execute_input":"2022-03-01T22:12:03.825088Z","iopub.status.idle":"2022-03-01T22:12:04.41869Z","shell.execute_reply.started":"2022-03-01T22:12:03.825029Z","shell.execute_reply":"2022-03-01T22:12:04.417829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above two cells, we immediately note the following:\n- There are 18 columns with missing values, out of which 7 have only 1 missing value.\n- Of the 18 columns, the number of missing values found in 8 columns (i.e., 974274 each) and the number found in 2 columns (i.e., 2598054) sum up to the length of the dataframe. \n\nCould there be a complimentary relationship, where columns in one set are filled in rows where those of the other set are missing?\nTo uncover this, we isolate the columns <b>M_WEATHER_PERCENTAGE and M_ZONE_START</b> and inspect the distribution of the missing vales across them. Due to the length of the dataframe, we slice a fraction of the data and visualise the distribution of missing values.","metadata":{}},{"cell_type":"code","source":"xdf = data.copy()\nxdf.M_RAIN_PERCENTAGE = np.where(xdf.M_RAIN_PERCENTAGE.isnull(),'1: Missing','1: Present')\nxdf.M_ZONE_START = np.where(xdf.M_ZONE_START.isnull(),'2: Missing','2: Present')\nxdf.M_ZONE_START.unique()\n\nplt.figure(figsize=(20,30))\ncount = 0\nfor i in range(1,16):\n    x1 = xdf[count:count+155].M_RAIN_PERCENTAGE\n    x2 = xdf[count:count+155].M_ZONE_START\n    index = range(count,count+155)\n    plt.subplot(5,3,i)\n    plt.plot(x1,index,'bo',markersize = 2,label='Rain percentage')\n    plt.plot(x2,index,'ro',markersize = 2,label='Time zone start')\n    plt.ylabel('Index') \n    plt.legend(loc='upper right')\n    plt.grid(False)\n    count+=155","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-01T22:12:54.881185Z","iopub.execute_input":"2022-03-01T22:12:54.882046Z","iopub.status.idle":"2022-03-01T22:12:59.100608Z","shell.execute_reply.started":"2022-03-01T22:12:54.882008Z","shell.execute_reply":"2022-03-01T22:12:59.098731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From these plots, we can generalise that the missing values in one column appear in rows where the others are filled. This holds true for the other columns across the two sets. As the insights build up, we willgain better claarity on how to prepare the data to achieve overall completeness and accuracy.","metadata":{}},{"cell_type":"markdown","source":"<br>\n<h4><b>4. Cleaning the Data</b></h4>","metadata":{}},{"cell_type":"markdown","source":"<b> (a) We will drop the following rows immediately </b>\n1. Rows where the number of forcast samples equals 0 as they provide no prediction at time t = 0\n2. Rows where the session type is unknown (0)\n3. Rows where the packet received shows a session type of NaN or 0\n4. Rows where the packet received is sent while the game is paused\n5. Rows where the packet received shows player is both spectating and playing online (inconsistency)\n6. Rows where marshal_zone_start or marshal_zone_flag is null, as these indicates gaps in the game","metadata":{}},{"cell_type":"code","source":"df = data.copy()\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:26:27.243783Z","iopub.execute_input":"2022-03-01T22:26:27.244069Z","iopub.status.idle":"2022-03-01T22:26:27.832053Z","shell.execute_reply.started":"2022-03-01T22:26:27.244037Z","shell.execute_reply":"2022-03-01T22:26:27.831338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(df[df['M_NUM_WEATHER_FORECAST_SAMPLES']==0].index, inplace=True)\ndf[df['M_NUM_WEATHER_FORECAST_SAMPLES']==0].count()","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:26:29.052187Z","iopub.execute_input":"2022-03-01T22:26:29.052901Z","iopub.status.idle":"2022-03-01T22:26:29.998881Z","shell.execute_reply.started":"2022-03-01T22:26:29.052852Z","shell.execute_reply":"2022-03-01T22:26:29.998157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:26:31.043936Z","iopub.execute_input":"2022-03-01T22:26:31.044737Z","iopub.status.idle":"2022-03-01T22:26:31.050415Z","shell.execute_reply.started":"2022-03-01T22:26:31.044693Z","shell.execute_reply":"2022-03-01T22:26:31.049629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(df[df['M_SESSION_TYPE']==0].index, inplace=True)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:26:32.30632Z","iopub.execute_input":"2022-03-01T22:26:32.307078Z","iopub.status.idle":"2022-03-01T22:26:33.115228Z","shell.execute_reply.started":"2022-03-01T22:26:32.307039Z","shell.execute_reply":"2022-03-01T22:26:33.114512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(df[df['M_GAME_PAUSED']==1].index, inplace=True)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:26:33.183858Z","iopub.execute_input":"2022-03-01T22:26:33.18406Z","iopub.status.idle":"2022-03-01T22:26:33.763962Z","shell.execute_reply.started":"2022-03-01T22:26:33.184035Z","shell.execute_reply":"2022-03-01T22:26:33.763136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(df[(df['M_IS_SPECTATING'] == 1) & (df['M_NETWORK_GAME'] == 1)].index, inplace=True)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:26:34.113024Z","iopub.execute_input":"2022-03-01T22:26:34.113805Z","iopub.status.idle":"2022-03-01T22:26:34.770483Z","shell.execute_reply.started":"2022-03-01T22:26:34.113763Z","shell.execute_reply":"2022-03-01T22:26:34.769765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(df[df['M_WEATHER_FORECAST_SAMPLES_M_SESSION_TYPE'].isnull()].index, inplace=True)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:26:34.863972Z","iopub.execute_input":"2022-03-01T22:26:34.864619Z","iopub.status.idle":"2022-03-01T22:26:35.801574Z","shell.execute_reply.started":"2022-03-01T22:26:34.864582Z","shell.execute_reply":"2022-03-01T22:26:35.800749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This last operation leaves the zone start and zone flag columns with NaN values as we can see below. We will therefore eliminate these columns in due time.","metadata":{}},{"cell_type":"code","source":"df['M_ZONE_FLAG'].isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:26:36.744581Z","iopub.execute_input":"2022-03-01T22:26:36.744843Z","iopub.status.idle":"2022-03-01T22:26:36.757118Z","shell.execute_reply.started":"2022-03-01T22:26:36.744811Z","shell.execute_reply":"2022-03-01T22:26:36.756351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br><b>(a) We will drop the following columns immediately:</b>\n1. Redundant columns not included with the packet, starting with gamehost and timestamp\n2. Redundant columns with single unique values (i.e., predominantly ID columns)\n3. Duplicated Columns\n4. Already Engineered Columns\n5. Forcast samples columns outside weather and %rainfall, since this is a weather forecast project\n\nWe start by aggregating the session duration and Session time left column to generate a new column representing the time delta in the game.","metadata":{}},{"cell_type":"code","source":"df['M_SESSION_TIME_SPENT'] = df['M_SESSION_DURATION'] - df['M_SESSION_TIME_LEFT']","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:26:40.570416Z","iopub.execute_input":"2022-03-01T22:26:40.570985Z","iopub.status.idle":"2022-03-01T22:26:40.58357Z","shell.execute_reply.started":"2022-03-01T22:26:40.570944Z","shell.execute_reply":"2022-03-01T22:26:40.582672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_col = ['GAMEHOST','TIMESTAMP', 'M_ZONE_FLAG', 'M_ZONE_START', 'M_SESSION_DURATION', \n           'M_SESSION_TIME_LEFT', 'M_WEATHER_FORECAST_SAMPLES_M_TRACK_TEMPERATURE', \n            'M_WEATHER_FORECAST_SAMPLES_M_AIR_TEMPERATURE', 'M_WEATHER_FORECAST_SAMPLES_M_SESSION_TYPE']\n\nfor col in df.columns:\n    if df[col].nunique()<2:\n        drop_col.append(col)\nprint(drop_col)\nlen(drop_col)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:27:26.644872Z","iopub.execute_input":"2022-03-01T22:27:26.645152Z","iopub.status.idle":"2022-03-01T22:27:27.378695Z","shell.execute_reply.started":"2022-03-01T22:27:26.645116Z","shell.execute_reply":"2022-03-01T22:27:27.37797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(drop_col, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:27:28.524Z","iopub.execute_input":"2022-03-01T22:27:28.52459Z","iopub.status.idle":"2022-03-01T22:27:29.105205Z","shell.execute_reply.started":"2022-03-01T22:27:28.524548Z","shell.execute_reply":"2022-03-01T22:27:29.104404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.count()","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:27:30.52956Z","iopub.execute_input":"2022-03-01T22:27:30.530059Z","iopub.status.idle":"2022-03-01T22:27:30.655013Z","shell.execute_reply.started":"2022-03-01T22:27:30.530018Z","shell.execute_reply":"2022-03-01T22:27:30.654151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().any().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:27:31.45419Z","iopub.execute_input":"2022-03-01T22:27:31.454907Z","iopub.status.idle":"2022-03-01T22:27:31.504054Z","shell.execute_reply.started":"2022-03-01T22:27:31.454864Z","shell.execute_reply":"2022-03-01T22:27:31.503369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>(c) Filling in the 6 columns containing single missing values</b>","metadata":{}},{"cell_type":"code","source":"fill_col = df.columns[df.isna().any() == True]\nfor col in fill_col:\n    df[col].fillna(df[col].mode()[0], inplace=True)\ndf.isna().any().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:27:33.432157Z","iopub.execute_input":"2022-03-01T22:27:33.432754Z","iopub.status.idle":"2022-03-01T22:27:33.626434Z","shell.execute_reply.started":"2022-03-01T22:27:33.432713Z","shell.execute_reply":"2022-03-01T22:27:33.625542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Shape of data before cleaning\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-01T22:27:34.360158Z","iopub.execute_input":"2022-03-01T22:27:34.360704Z","iopub.status.idle":"2022-03-01T22:27:34.366481Z","shell.execute_reply.started":"2022-03-01T22:27:34.360666Z","shell.execute_reply":"2022-03-01T22:27:34.365608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Shape of data after cleaning\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-02T04:27:16.728030Z","iopub.execute_input":"2022-03-02T04:27:16.728390Z","iopub.status.idle":"2022-03-02T04:27:16.806426Z","shell.execute_reply.started":"2022-03-02T04:27:16.728277Z","shell.execute_reply":"2022-03-02T04:27:16.805467Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df.to_csv('cleaned_one.csv', encoding='utf-8', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}