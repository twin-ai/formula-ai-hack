{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Learning to Hack Game Weather Conditions**\n#### An End-to-End Machine Learning Workflow for Formula One In-Game Weather Forecasting\n#### Team Twin AI ####\n\n***Contributors to this Notebook:*** Zion Pibowei, Temitayo Adejuyigbe, Anosike Chimaobi Nice","metadata":{}},{"cell_type":"markdown","source":"## Background\n\nFormula 1 is one of the most competitive sports in the world. Engineers and technicians from every team use weather radar screens, provided by Ubimet to the teams, which allows them to track the current weather and make predictions during the race. Race engineers relay precise information to drivers, including:\n\n- How many minutes until it starts raining\n- Intensity of the rain\n- Which corner will be hit first by the rain\n- Duration of the rain\n\nPoints, and even races sometimes, are won and lost based on making sense of what the weather is going to do during a race, and being prepared as a team to act accordingly.\n\nTherefore, weather forecasting takes a big part on the possible outcome of a race.\n\nSimilarly, F1 2021, the official Formula 1 videogame developed by Codemasters, uses a physics engine that behaves like the real world.","metadata":{}},{"cell_type":"markdown","source":"## The Challenge\n\nIn this challenge, we are required to analyse historical weather data from the RedBull Racing eSports team to build a high-performing model that is able to make accurate weather predictions/forecasts. Our objective is to predict the weather type 5, 10, 15, 30 and 60 minutes after a timestamp, and the corresponding rain percentage probability at each time. \n\n***Our solution is divided into 4 Sections, each constituting a workflow on its own:***\n\n- Part I: Initial Data Analysis and Preprocessing\n- Part II: EDA and Feature Selection\n- Part III: Modelling Methodology\n- Part IV: Predictions and Exporting","metadata":{}},{"cell_type":"markdown","source":"## Part I: Initial Data Analysis and Preprocessing\n<h4><b>Overview</b></h4>\n\nThis is the IDA and Preprocessing component of our solution to the FormulaAI Hack 2022 Competition. The workflow for this notebook is outlined as follows: \n- Getting the Data\n- First Insights: Making Sense of the Data\n- Data Integrity Assessments\n- Cleaning the Data\n","metadata":{"execution":{"iopub.execute_input":"2022-03-01T21:00:12.52902Z","iopub.status.busy":"2022-03-01T21:00:12.528716Z","iopub.status.idle":"2022-03-01T21:00:12.536057Z","shell.execute_reply":"2022-03-01T21:00:12.535013Z","shell.execute_reply.started":"2022-03-01T21:00:12.528991Z"}}},{"cell_type":"code","source":"import os\n\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', None, 'display.max_rows', 100)\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import figure\n%matplotlib inline\n\nimport seaborn as sns\nsns.set_context('notebook')\nsns.set_style('whitegrid')\nsns.set_palette('Blues_r')\n\n!conda install -c conda-forge deepchecks -y\nimport deepchecks as dc\nfrom deepchecks.checks.integrity.is_single_value import IsSingleValue\nfrom deepchecks.checks.integrity.data_duplicates import DataDuplicates\nfrom deepchecks.checks import DataDuplicates\nfrom deepchecks.checks.integrity import LabelAmbiguity\nfrom deepchecks.base import Dataset, Suite\n\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\n\nimport random\nimport time\nfrom datetime import datetime\n\nimport warnings\n# warnings.filterwarnings('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:35:37.976774Z","iopub.execute_input":"2022-03-04T11:35:37.977709Z","iopub.status.idle":"2022-03-04T11:38:39.094355Z","shell.execute_reply.started":"2022-03-04T11:35:37.977664Z","shell.execute_reply":"2022-03-04T11:38:39.093361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!conda install -c conda-forge deepchecks -y","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:38:39.096369Z","iopub.execute_input":"2022-03-04T11:38:39.096853Z","iopub.status.idle":"2022-03-04T11:38:39.100817Z","shell.execute_reply.started":"2022-03-04T11:38:39.096817Z","shell.execute_reply":"2022-03-04T11:38:39.099803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:38:39.103189Z","iopub.execute_input":"2022-03-04T11:38:39.103962Z","iopub.status.idle":"2022-03-04T11:38:39.137031Z","shell.execute_reply.started":"2022-03-04T11:38:39.103918Z","shell.execute_reply":"2022-03-04T11:38:39.135967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading the CSV file\ndata = pd.read_csv('../input/formulaaihackathon2022/weather.csv',low_memory=False)\n#data = pd.read_csv('weather.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:38:39.138897Z","iopub.execute_input":"2022-03-04T11:38:39.13921Z","iopub.status.idle":"2022-03-04T11:39:14.168189Z","shell.execute_reply.started":"2022-03-04T11:38:39.139158Z","shell.execute_reply":"2022-03-04T11:39:14.167312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<h4><b>1. First Insights: Making Sense of the Data </b></h4>","metadata":{}},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:14.170306Z","iopub.execute_input":"2022-03-04T11:39:14.17058Z","iopub.status.idle":"2022-03-04T11:39:14.176315Z","shell.execute_reply.started":"2022-03-04T11:39:14.170551Z","shell.execute_reply":"2022-03-04T11:39:14.175449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info() #check the overall information about the dataset","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-04T11:39:14.177913Z","iopub.execute_input":"2022-03-04T11:39:14.178551Z","iopub.status.idle":"2022-03-04T11:39:14.202429Z","shell.execute_reply.started":"2022-03-04T11:39:14.178505Z","shell.execute_reply":"2022-03-04T11:39:14.201513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:14.20384Z","iopub.execute_input":"2022-03-04T11:39:14.204272Z","iopub.status.idle":"2022-03-04T11:39:14.23745Z","shell.execute_reply.started":"2022-03-04T11:39:14.204226Z","shell.execute_reply":"2022-03-04T11:39:14.23687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>In this dataframe, we see that the last column is an unnamed column. Before we drop it, let us inspect the proportion of NaN values it contains.<br>","metadata":{}},{"cell_type":"code","source":"print('Missing values:',data['Unnamed: 58'].isnull().sum())\nprint('Proportion of missing values: {}%'.format(data['Unnamed: 58'].isnull().sum()/data.shape[0]*100))","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:14.238604Z","iopub.execute_input":"2022-03-04T11:39:14.238979Z","iopub.status.idle":"2022-03-04T11:39:14.259989Z","shell.execute_reply.started":"2022-03-04T11:39:14.238933Z","shell.execute_reply":"2022-03-04T11:39:14.259401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clearly, the unnamed column is entirely filled with missing values and, as such, has no impact in our workflow. It's presence in the data is most likely due to encoding. Thus, the first treatment to our data is to drop this column.","metadata":{}},{"cell_type":"code","source":"data.drop('Unnamed: 58', axis = 1, inplace = True)\ndata.shape[1]","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:14.261398Z","iopub.execute_input":"2022-03-04T11:39:14.261896Z","iopub.status.idle":"2022-03-04T11:39:14.745301Z","shell.execute_reply.started":"2022-03-04T11:39:14.261852Z","shell.execute_reply":"2022-03-04T11:39:14.74452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's obtain summary statistics for our data ","metadata":{}},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:14.746642Z","iopub.execute_input":"2022-03-04T11:39:14.746936Z","iopub.status.idle":"2022-03-04T11:39:19.116217Z","shell.execute_reply.started":"2022-03-04T11:39:14.746897Z","shell.execute_reply":"2022-03-04T11:39:19.115407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br><b>At a glance:</b>\n- A quick inspection of the count row shows us that a number of columns contain missing values, ranging from small to large.\n- A quick inspection of the standard deviation shows us that some columns have zero variance, indicating that <b>each of these columns contain ONLY ONE distinct value</b>. Typically, variables whose standard deviations tend to zero have fewer distinct values.\n- A quick inspection of the min and max shows us that the very columns having 0 standard deviation <b>contain equal values of minimum and maximum</b>, validating our claim that these columns have only one distinct value.\n\nIn this project, we will carry out exhaustive analysis of the data to address the implictations of the forgoing discoveries.</b>\n","metadata":{}},{"cell_type":"markdown","source":"<h4><b>2. Data Integrity Assessments</b></h4>\n<p>In this section, we will investigate the integrity of the data and uncover any data quality issues that may be present. The insights we obtain in this section will guide us on how to resolve these issues pragmatically in the next section.</p>","metadata":{}},{"cell_type":"markdown","source":"<p><b>(a) Unique Values</b></p>\n\nOur ultimate goal is to build <b>a model that learns the evolution of weather conditions over time</b>. Therefore, we are interested in columns that show variation of values over time. Columns that contain only one unique value <b>may not provide predictive power for the model</b>. We will validate this assumption when we implement feature contribution checks ahead of our model methodology.\n<p> First, we make a general inspection of the number of unique values contained in all the columns.</p>","metadata":{}},{"cell_type":"code","source":"data.nunique(axis=0).sort_values().to_frame() #check for unique values and sort them into frames","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-04T11:39:19.11745Z","iopub.execute_input":"2022-03-04T11:39:19.1177Z","iopub.status.idle":"2022-03-04T11:39:20.737664Z","shell.execute_reply.started":"2022-03-04T11:39:19.117672Z","shell.execute_reply":"2022-03-04T11:39:20.737014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above result, we can see that there are 7 columns that contain only one unique value. Below, we obtain further information about what these exact values are.","metadata":{}},{"cell_type":"code","source":"sv = IsSingleValue()\nsv.run(data)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:20.739088Z","iopub.execute_input":"2022-03-04T11:39:20.739534Z","iopub.status.idle":"2022-03-04T11:39:23.534873Z","shell.execute_reply.started":"2022-03-04T11:39:20.739494Z","shell.execute_reply":"2022-03-04T11:39:23.533955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<p><b>(b) Data Duplicates<b></p>\nWe need to run a duplicate check to find if there are multiple instances of identical samples in our dataset. One reason is that duplicates could be an indicator for a problem in the data pipeline that requires attention. The other is that they can potentially increase the weight that a machine learning model gives to samples. ","metadata":{}},{"cell_type":"code","source":"print('Proportion of duplicates: {}%'.format(len(data[data.duplicated()])/data.shape[0]*100))\ndata[data.duplicated()] #check for average duplicate values in the dataset","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-04T11:39:23.536495Z","iopub.execute_input":"2022-03-04T11:39:23.537157Z","iopub.status.idle":"2022-03-04T11:39:36.639809Z","shell.execute_reply.started":"2022-03-04T11:39:23.537111Z","shell.execute_reply":"2022-03-04T11:39:36.638744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, this is only partially informative. We only know that 2057230 samples, representing ~57% of the data, are duplicated. But this doesn't tell us the number of times each example of duplicate data appears. We will obtain the desired information by implementing the following additional checks.","metadata":{}},{"cell_type":"code","source":"#from deepchecks.checks import DataDuplicates\n#DataDuplicates().run(data)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:36.643989Z","iopub.execute_input":"2022-03-04T11:39:36.644248Z","iopub.status.idle":"2022-03-04T11:39:36.648398Z","shell.execute_reply.started":"2022-03-04T11:39:36.644217Z","shell.execute_reply":"2022-03-04T11:39:36.647556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can summarise this check by defining a check condition that sets the baseline of duplicate ratio as 0. This will expose any violation to the condition and reveal the present duplicate ratio.","metadata":{}},{"cell_type":"code","source":"#check = DataDuplicates()\n#check.add_condition_ratio_not_greater_than(0)\n#result = check.run(data)\n#result.show(show_additional_outputs=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:36.649703Z","iopub.execute_input":"2022-03-04T11:39:36.650154Z","iopub.status.idle":"2022-03-04T11:39:36.659234Z","shell.execute_reply.started":"2022-03-04T11:39:36.650107Z","shell.execute_reply":"2022-03-04T11:39:36.658455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are interested in knowing whether the duplicates observed here were intentionally intended to be part of the data. However, if this is an hidden issue weâ€™re not expecting to occur, then we will need to resolve it. We will revisit this in the EDA component of our workflow.","metadata":{}},{"cell_type":"markdown","source":"<br>\n<p><b>(c) Label Ambiguity</b></p>","metadata":{}},{"cell_type":"markdown","source":"We would also like to check whether there are identical samples in the data with different labels. This alerts us to further verify whether or not the data was mislabelled, as mislabelled data could confuse the model and lead to lower model performance.","metadata":{}},{"cell_type":"code","source":"#label_ambig = Dataset(data, label='M_WEATHER')\n#LabelAmbiguity().run(label_ambig)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:36.660345Z","iopub.execute_input":"2022-03-04T11:39:36.660958Z","iopub.status.idle":"2022-03-04T11:39:36.66941Z","shell.execute_reply.started":"2022-03-04T11:39:36.660916Z","shell.execute_reply":"2022-03-04T11:39:36.668796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, we summarise this check by defining a check condition that sets the baseline of ambiguous sample ratio as 0. This will expose any violation to the condition and reveal the present ambiguous sample ratio.","metadata":{}},{"cell_type":"code","source":"#check = LabelAmbiguity()\n#check.add_condition_ambiguous_sample_ratio_not_greater_than(0)\n#result = check.run(label_ambig)\n#result.show(show_additional_outputs=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:36.670565Z","iopub.execute_input":"2022-03-04T11:39:36.670926Z","iopub.status.idle":"2022-03-04T11:39:36.679909Z","shell.execute_reply.started":"2022-03-04T11:39:36.670897Z","shell.execute_reply":"2022-03-04T11:39:36.679255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Indeed, we observe that there are no identical samples with different labels.","metadata":{}},{"cell_type":"markdown","source":"<br>\n<p><b>(d) Missing Values</b></p>","metadata":{}},{"cell_type":"code","source":"data.isna().sum().sort_values().to_frame() #check for missing values and sort them into frames","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:36.681177Z","iopub.execute_input":"2022-03-04T11:39:36.68201Z","iopub.status.idle":"2022-03-04T11:39:37.330052Z","shell.execute_reply.started":"2022-03-04T11:39:36.681975Z","shell.execute_reply":"2022-03-04T11:39:37.32914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.notna().sum().sort_values().to_frame() #check for non-missing values and sort them into frames","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:37.331528Z","iopub.execute_input":"2022-03-04T11:39:37.331858Z","iopub.status.idle":"2022-03-04T11:39:38.041462Z","shell.execute_reply.started":"2022-03-04T11:39:37.331815Z","shell.execute_reply":"2022-03-04T11:39:38.040647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above two cells, we immediately note the following:\n- There are 18 columns with missing values, out of which 7 have only 1 missing value.\n- Of the 18 columns, the number of missing values found in 8 columns (i.e., 974274 each) and the number found in 2 columns (i.e., 2598054) sum up to the length of the dataframe. \n\nCould there be a complimentary relationship, where columns in one set are filled in rows where those of the other set are missing?\nTo uncover this, we isolate the columns <b>M_WEATHER_PERCENTAGE and M_ZONE_START</b> and inspect the distribution of the missing vales across them. Due to the length of the dataframe, we slice a fraction of the data and visualise the distribution of missing values.","metadata":{}},{"cell_type":"code","source":"xdf = data.copy()\nxdf.M_RAIN_PERCENTAGE = np.where(xdf.M_RAIN_PERCENTAGE.isnull(),'1: Missing','1: Present')\nxdf.M_ZONE_START = np.where(xdf.M_ZONE_START.isnull(),'2: Missing','2: Present')\nxdf.M_ZONE_START.unique()\n\nplt.figure(figsize=(20,30))\ncount = 0\nfor i in range(1,16):\n    x1 = xdf[count:count+155].M_RAIN_PERCENTAGE\n    x2 = xdf[count:count+155].M_ZONE_START\n    index = range(count,count+155)\n    plt.subplot(5,3,i)\n    plt.plot(x1,index,'bo',markersize = 2,label='Rain percentage')\n    plt.plot(x2,index,'ro',markersize = 2,label='Time zone start')\n    plt.ylabel('Index') \n    plt.legend(loc='upper right')\n    plt.grid(False)\n    count+=155","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-04T11:39:38.042982Z","iopub.execute_input":"2022-03-04T11:39:38.043307Z","iopub.status.idle":"2022-03-04T11:39:44.368936Z","shell.execute_reply.started":"2022-03-04T11:39:38.043266Z","shell.execute_reply":"2022-03-04T11:39:44.368237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From these plots, we can generalise that the missing values in one column appear in rows where the others are filled. This holds true for the other columns across the two sets. As the insights build up, we willgain better claarity on how to prepare the data to achieve overall completeness and accuracy.","metadata":{}},{"cell_type":"markdown","source":"<br>\n<h4><b>4. Cleaning the Data</b></h4>","metadata":{}},{"cell_type":"markdown","source":"<b> (a) We will drop the following rows immediately </b>\n1. Rows where the number of forcast samples equals 0 as they provide no prediction at time t = 0\n2. Rows where the session type is unknown (0)\n3. Rows where the packet received shows a session type of NaN or 0\n4. Rows where the packet received is sent while the game is paused\n5. Rows where the packet received shows player is both spectating and playing online (inconsistency)\n6. Rows where marshal_zone_start or marshal_zone_flag is null, as these indicates gaps in the game","metadata":{}},{"cell_type":"code","source":"df = data.copy()\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:44.370221Z","iopub.execute_input":"2022-03-04T11:39:44.370886Z","iopub.status.idle":"2022-03-04T11:39:45.28355Z","shell.execute_reply.started":"2022-03-04T11:39:44.370848Z","shell.execute_reply":"2022-03-04T11:39:45.282532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(df[df['M_NUM_WEATHER_FORECAST_SAMPLES']==0].index, inplace=True)\ndf[df['M_NUM_WEATHER_FORECAST_SAMPLES']==0].count()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:45.285122Z","iopub.execute_input":"2022-03-04T11:39:45.285525Z","iopub.status.idle":"2022-03-04T11:39:47.66192Z","shell.execute_reply.started":"2022-03-04T11:39:45.285461Z","shell.execute_reply":"2022-03-04T11:39:47.660845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:47.663767Z","iopub.execute_input":"2022-03-04T11:39:47.664087Z","iopub.status.idle":"2022-03-04T11:39:47.671331Z","shell.execute_reply.started":"2022-03-04T11:39:47.664045Z","shell.execute_reply":"2022-03-04T11:39:47.67037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(df[df['M_SESSION_TYPE']==0].index, inplace=True)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:47.673165Z","iopub.execute_input":"2022-03-04T11:39:47.673829Z","iopub.status.idle":"2022-03-04T11:39:48.686797Z","shell.execute_reply.started":"2022-03-04T11:39:47.673785Z","shell.execute_reply":"2022-03-04T11:39:48.685907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(df[df['M_GAME_PAUSED']==1].index, inplace=True)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:48.687834Z","iopub.execute_input":"2022-03-04T11:39:48.688056Z","iopub.status.idle":"2022-03-04T11:39:49.37747Z","shell.execute_reply.started":"2022-03-04T11:39:48.688014Z","shell.execute_reply":"2022-03-04T11:39:49.37665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(df[(df['M_IS_SPECTATING'] == 1) & (df['M_NETWORK_GAME'] == 1)].index, inplace=True)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:49.378902Z","iopub.execute_input":"2022-03-04T11:39:49.379119Z","iopub.status.idle":"2022-03-04T11:39:50.171896Z","shell.execute_reply.started":"2022-03-04T11:39:49.379093Z","shell.execute_reply":"2022-03-04T11:39:50.17102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(df[df['M_WEATHER_FORECAST_SAMPLES_M_SESSION_TYPE'].isnull()].index, inplace=True)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:50.173114Z","iopub.execute_input":"2022-03-04T11:39:50.173548Z","iopub.status.idle":"2022-03-04T11:39:51.305543Z","shell.execute_reply.started":"2022-03-04T11:39:50.173516Z","shell.execute_reply":"2022-03-04T11:39:51.304681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This last operation leaves the zone start and zone flag columns with NaN values as we can see below. We will therefore eliminate these columns in due time.","metadata":{}},{"cell_type":"code","source":"df['M_ZONE_FLAG'].isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:51.307193Z","iopub.execute_input":"2022-03-04T11:39:51.307528Z","iopub.status.idle":"2022-03-04T11:39:51.31891Z","shell.execute_reply.started":"2022-03-04T11:39:51.307469Z","shell.execute_reply":"2022-03-04T11:39:51.318179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br><b>(a) We will drop the following columns immediately:</b>\n1. Redundant columns not included with the packet, starting with gamehost and timestamp\n2. Redundant columns with single unique values (i.e., predominantly ID columns)\n3. Duplicated Columns\n4. Already Engineered Columns\n5. Forcast samples columns outside weather and %rainfall, since this is a weather forecast project\n\nWe start by aggregating the session duration and Session time left column to generate a new column representing the time delta in the game.","metadata":{}},{"cell_type":"code","source":"df['M_SESSION_TIME_SPENT'] = df['M_SESSION_DURATION'] - df['M_SESSION_TIME_LEFT']","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:51.320298Z","iopub.execute_input":"2022-03-04T11:39:51.32116Z","iopub.status.idle":"2022-03-04T11:39:51.335902Z","shell.execute_reply.started":"2022-03-04T11:39:51.321115Z","shell.execute_reply":"2022-03-04T11:39:51.33489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_col = ['GAMEHOST','TIMESTAMP', 'M_ZONE_FLAG', 'M_ZONE_START', 'M_SESSION_DURATION', \n           'M_SESSION_TIME_LEFT', 'M_WEATHER_FORECAST_SAMPLES_M_TRACK_TEMPERATURE', \n            'M_WEATHER_FORECAST_SAMPLES_M_AIR_TEMPERATURE', 'M_WEATHER_FORECAST_SAMPLES_M_SESSION_TYPE']\n\nfor col in df.columns:\n    if df[col].nunique()<2:\n        drop_col.append(col)\nprint(drop_col)\nlen(drop_col)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:51.337301Z","iopub.execute_input":"2022-03-04T11:39:51.337897Z","iopub.status.idle":"2022-03-04T11:39:52.248771Z","shell.execute_reply.started":"2022-03-04T11:39:51.337859Z","shell.execute_reply":"2022-03-04T11:39:52.24777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(drop_col, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:52.249981Z","iopub.execute_input":"2022-03-04T11:39:52.250247Z","iopub.status.idle":"2022-03-04T11:39:52.926241Z","shell.execute_reply.started":"2022-03-04T11:39:52.250189Z","shell.execute_reply":"2022-03-04T11:39:52.92537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.count()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:52.92754Z","iopub.execute_input":"2022-03-04T11:39:52.92777Z","iopub.status.idle":"2022-03-04T11:39:53.078837Z","shell.execute_reply.started":"2022-03-04T11:39:52.927744Z","shell.execute_reply":"2022-03-04T11:39:53.077867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().any().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:53.079956Z","iopub.execute_input":"2022-03-04T11:39:53.080177Z","iopub.status.idle":"2022-03-04T11:39:53.126865Z","shell.execute_reply.started":"2022-03-04T11:39:53.08015Z","shell.execute_reply":"2022-03-04T11:39:53.125864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>(c) Filling in the 6 columns containing single missing values</b>","metadata":{}},{"cell_type":"code","source":"fill_col = df.columns[df.isna().any() == True]\nfor col in fill_col:\n    df[col].fillna(df[col].mode()[0], inplace=True)\ndf.isna().any().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:53.128203Z","iopub.execute_input":"2022-03-04T11:39:53.128437Z","iopub.status.idle":"2022-03-04T11:39:53.348602Z","shell.execute_reply.started":"2022-03-04T11:39:53.128397Z","shell.execute_reply":"2022-03-04T11:39:53.347973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Shape of data before cleaning\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:53.350188Z","iopub.execute_input":"2022-03-04T11:39:53.350425Z","iopub.status.idle":"2022-03-04T11:39:53.355493Z","shell.execute_reply.started":"2022-03-04T11:39:53.350395Z","shell.execute_reply":"2022-03-04T11:39:53.35481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Shape of data after cleaning\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:53.356702Z","iopub.execute_input":"2022-03-04T11:39:53.356929Z","iopub.status.idle":"2022-03-04T11:39:53.371591Z","shell.execute_reply.started":"2022-03-04T11:39:53.356901Z","shell.execute_reply":"2022-03-04T11:39:53.370584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df.to_csv('cleaned_one.csv', encoding='utf-8', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:53.373096Z","iopub.execute_input":"2022-03-04T11:39:53.374104Z","iopub.status.idle":"2022-03-04T11:39:53.379982Z","shell.execute_reply.started":"2022-03-04T11:39:53.374032Z","shell.execute_reply":"2022-03-04T11:39:53.379127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"  ","metadata":{}},{"cell_type":"markdown","source":"## Part II: Exploratory Data Analysis and Feature Selection\n<h4><b>Overview</b></h4>\n\nThis is the EDA and Feature Egineering component of our solution to the FormulaAI Hack 2022 Competition. The workflow for this notebook is outlined as follows: \n- Multivariate Exploratory Analysis\n- Feature Importance and Selection","metadata":{}},{"cell_type":"code","source":"import os\n\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', None, 'display.max_rows', 100)\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nimport deepchecks as dc\nfrom deepchecks.checks.methodology import SingleFeatureContribution\nfrom deepchecks.base import Dataset\n\n#from sklearnex import patch_sklearn\n#patch_sklearn()\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.feature_selection import GenericUnivariateSelect\nfrom sklearn.preprocessing import scale, StandardScaler, RobustScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom scipy import stats\n\nimport random\nimport time\nfrom datetime import datetime\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:53.382042Z","iopub.execute_input":"2022-03-04T11:39:53.382622Z","iopub.status.idle":"2022-03-04T11:39:53.419837Z","shell.execute_reply.started":"2022-03-04T11:39:53.382531Z","shell.execute_reply":"2022-03-04T11:39:53.418827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data= pd.read_csv('cleaned_one.csv')\n#data.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:53.420946Z","iopub.execute_input":"2022-03-04T11:39:53.421175Z","iopub.status.idle":"2022-03-04T11:39:53.42555Z","shell.execute_reply.started":"2022-03-04T11:39:53.421132Z","shell.execute_reply":"2022-03-04T11:39:53.424544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data.drop('Unnamed: 0', axis =1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:53.426613Z","iopub.execute_input":"2022-03-04T11:39:53.427201Z","iopub.status.idle":"2022-03-04T11:39:53.436362Z","shell.execute_reply.started":"2022-03-04T11:39:53.427163Z","shell.execute_reply":"2022-03-04T11:39:53.435467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df = data.copy()\nprint(df.shape)\ndf.isna().values.any()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:53.443471Z","iopub.execute_input":"2022-03-04T11:39:53.443915Z","iopub.status.idle":"2022-03-04T11:39:53.547707Z","shell.execute_reply.started":"2022-03-04T11:39:53.443865Z","shell.execute_reply":"2022-03-04T11:39:53.546828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<h4><b>1. Multivariate Exploratory Analysis</b></h4>\n<p> Our first step is to visualise and understand the correlation that exists between the features in the data","metadata":{}},{"cell_type":"code","source":"corrmat = df.corr()\n#f, ax = plt.subplots(figsize=(16, 12))\n#sns.heatmap(corrmat, vmax=.8, annot=True, square=True, fmt='.2f')\n\nplt.figure(figsize=(80,80))\nsns.set(font_scale=2.5)\nsns.heatmap(corrmat, annot=True, fmt='.1f', cmap='YlGnBu')","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:39:53.549165Z","iopub.execute_input":"2022-03-04T11:39:53.550679Z","iopub.status.idle":"2022-03-04T11:40:11.037813Z","shell.execute_reply.started":"2022-03-04T11:39:53.550618Z","shell.execute_reply":"2022-03-04T11:40:11.0355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above plot is too cluttered. If we zoom further in to the above plot, we find that M_WEEKEND_LINK_IDENTIFIER, M_SEASON_LINK_IDENTIFIER and M_SESSION_LINK_IDENTIFIER show\nperfect correleration of 1.","metadata":{}},{"cell_type":"code","source":"xtremilar_col = ['M_WEEKEND_LINK_IDENTIFIER', 'M_SEASON_LINK_IDENTIFIER', 'M_SESSION_LINK_IDENTIFIER']","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:11.039034Z","iopub.execute_input":"2022-03-04T11:40:11.03927Z","iopub.status.idle":"2022-03-04T11:40:11.04321Z","shell.execute_reply.started":"2022-03-04T11:40:11.03924Z","shell.execute_reply":"2022-03-04T11:40:11.04266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrmat = df[xtremilar_col ].corr()\nf, ax = plt.subplots(figsize=(15, 10))\nsns.heatmap(corrmat, vmax=.4, annot=True, square=True, fmt='.1f',)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:11.044249Z","iopub.execute_input":"2022-03-04T11:40:11.045013Z","iopub.status.idle":"2022-03-04T11:40:11.587435Z","shell.execute_reply.started":"2022-03-04T11:40:11.044972Z","shell.execute_reply":"2022-03-04T11:40:11.5866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in xtremilar_col:\n    print(col,'-->',df[col].nunique())","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:11.588725Z","iopub.execute_input":"2022-03-04T11:40:11.588964Z","iopub.status.idle":"2022-03-04T11:40:11.632497Z","shell.execute_reply.started":"2022-03-04T11:40:11.588933Z","shell.execute_reply":"2022-03-04T11:40:11.631445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>For the heatmap to have indicated a correlation of 1.0, then it means one or 2 columns will most likely contain all of the values in others. We will perform a groupby on each of them to identify how many unique values in one column correspond to each unique value in another column.","metadata":{}},{"cell_type":"code","source":"df[xtremilar_col].groupby('M_WEEKEND_LINK_IDENTIFIER').nunique()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:11.633643Z","iopub.execute_input":"2022-03-04T11:40:11.634245Z","iopub.status.idle":"2022-03-04T11:40:11.8713Z","shell.execute_reply.started":"2022-03-04T11:40:11.634211Z","shell.execute_reply":"2022-03-04T11:40:11.870531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[xtremilar_col].groupby('M_SEASON_LINK_IDENTIFIER').nunique()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:11.872526Z","iopub.execute_input":"2022-03-04T11:40:11.872836Z","iopub.status.idle":"2022-03-04T11:40:12.10785Z","shell.execute_reply.started":"2022-03-04T11:40:11.872803Z","shell.execute_reply":"2022-03-04T11:40:12.106971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[xtremilar_col].groupby('M_SESSION_LINK_IDENTIFIER').nunique()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:12.109294Z","iopub.execute_input":"2022-03-04T11:40:12.109701Z","iopub.status.idle":"2022-03-04T11:40:12.337989Z","shell.execute_reply.started":"2022-03-04T11:40:12.109654Z","shell.execute_reply":"2022-03-04T11:40:12.337265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the first result, we find that only the season link ID occurs exactly once in the weekend ID groups. However, the second result shows that the weekend ID occurs more than once in some season ID groups. Therefore, weekend ID is a superset of season ID, indicating that weekend ID captures all the information that weekend ID presents. \n\nFinally, we see that in the third result, both season ID and weekend ID occur exactly once in the session ID group, indicating that session ID captures all the informations that both season ID and weekend ID presents. This makes sense considering that whether the games are played in a weekend, a season, or otherwise, each game is played in session. This provides the justification for which we will be dropping both the season and weekend IDs.","metadata":{}},{"cell_type":"code","source":"df.drop(xtremilar_col[:-1], axis=1, inplace=True)\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:12.339397Z","iopub.execute_input":"2022-03-04T11:40:12.339962Z","iopub.status.idle":"2022-03-04T11:40:12.567106Z","shell.execute_reply.started":"2022-03-04T11:40:12.339895Z","shell.execute_reply":"2022-03-04T11:40:12.566139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's zoom into the heatmap to observe the top 20 most correlated features with weather","metadata":{}},{"cell_type":"code","source":"#### top 20 most correlated features with Weather (correlation matrix) \n\nf, ax = plt.subplots(figsize=(25,20))\nk = 20 #number of variables for heatmap\ncols = df.corr().nlargest(k, 'M_WEATHER')['M_WEATHER'].index\ncm = np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.1f', cmap='YlGnBu', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:12.568573Z","iopub.execute_input":"2022-03-04T11:40:12.568887Z","iopub.status.idle":"2022-03-04T11:40:23.176668Z","shell.execute_reply.started":"2022-03-04T11:40:12.568845Z","shell.execute_reply":"2022-03-04T11:40:23.175695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Zooming further, we notice a correlated submatrix of 7 features which showed perfect correleration of 1. This block contains five of the seven assist features and the two dynamic racing line features. \n\nOur assumption is that the other two assists features that do not make it to this block, namely steering assist and braking assist, could be due to a much smaller proportion of players using these assists, as more players typically have the experience to play without these 2 supports. ","metadata":{}},{"cell_type":"code","source":"xtremilar_col2 = [\n'M_DYNAMIC_RACING_LINE_TYPE'\n,'M_DYNAMIC_RACING_LINE'\n,'M_PIT_RELEASE_ASSIST'\n,'M_ERSASSIST'\n,'M_PIT_ASSIST'\n,'M_GEARBOX_ASSIST'\n,'M_STEERING_ASSIST'\n,'M_BRAKING_ASSIST'\n,'M_DRSASSIST']","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:23.177675Z","iopub.execute_input":"2022-03-04T11:40:23.177889Z","iopub.status.idle":"2022-03-04T11:40:23.182321Z","shell.execute_reply.started":"2022-03-04T11:40:23.177862Z","shell.execute_reply":"2022-03-04T11:40:23.181527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrmat2 = df[xtremilar_col2].corr()\nf, ax = plt.subplots(figsize=(16, 12))\nsns.heatmap(corrmat2, vmax=1, annot=True, square=True, fmt='.1f', cmap='YlGnBu')","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:23.183822Z","iopub.execute_input":"2022-03-04T11:40:23.184293Z","iopub.status.idle":"2022-03-04T11:40:24.636311Z","shell.execute_reply.started":"2022-03-04T11:40:23.184193Z","shell.execute_reply":"2022-03-04T11:40:24.634562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we have added the two remaining assists and we see they are not as correlated. We begin by performing a groupby on the columns in the list, one column at a time.","metadata":{}},{"cell_type":"code","source":"df[xtremilar_col2].groupby('M_DYNAMIC_RACING_LINE_TYPE').nunique()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:24.637835Z","iopub.execute_input":"2022-03-04T11:40:24.638939Z","iopub.status.idle":"2022-03-04T11:40:25.325135Z","shell.execute_reply.started":"2022-03-04T11:40:24.638855Z","shell.execute_reply":"2022-03-04T11:40:25.324538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[xtremilar_col2].groupby('M_DYNAMIC_RACING_LINE').nunique()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:25.326221Z","iopub.execute_input":"2022-03-04T11:40:25.326883Z","iopub.status.idle":"2022-03-04T11:40:26.015092Z","shell.execute_reply.started":"2022-03-04T11:40:25.32685Z","shell.execute_reply":"2022-03-04T11:40:26.014228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[xtremilar_col2].groupby('M_ERSASSIST').nunique()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:26.016622Z","iopub.execute_input":"2022-03-04T11:40:26.017212Z","iopub.status.idle":"2022-03-04T11:40:26.703199Z","shell.execute_reply.started":"2022-03-04T11:40:26.017131Z","shell.execute_reply":"2022-03-04T11:40:26.702604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[xtremilar_col2].groupby('M_PIT_ASSIST').nunique()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:26.704265Z","iopub.execute_input":"2022-03-04T11:40:26.704596Z","iopub.status.idle":"2022-03-04T11:40:27.402119Z","shell.execute_reply.started":"2022-03-04T11:40:26.704568Z","shell.execute_reply":"2022-03-04T11:40:27.401556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[xtremilar_col2].groupby('M_GEARBOX_ASSIST').nunique()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:27.403177Z","iopub.execute_input":"2022-03-04T11:40:27.403528Z","iopub.status.idle":"2022-03-04T11:40:28.101009Z","shell.execute_reply.started":"2022-03-04T11:40:27.403489Z","shell.execute_reply":"2022-03-04T11:40:28.100432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[xtremilar_col2].groupby('M_STEERING_ASSIST').nunique()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:28.102169Z","iopub.execute_input":"2022-03-04T11:40:28.102408Z","iopub.status.idle":"2022-03-04T11:40:28.801743Z","shell.execute_reply.started":"2022-03-04T11:40:28.10237Z","shell.execute_reply":"2022-03-04T11:40:28.800695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[xtremilar_col2].groupby('M_BRAKING_ASSIST').nunique()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:28.803257Z","iopub.execute_input":"2022-03-04T11:40:28.803563Z","iopub.status.idle":"2022-03-04T11:40:29.494861Z","shell.execute_reply.started":"2022-03-04T11:40:28.803532Z","shell.execute_reply":"2022-03-04T11:40:29.493886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[xtremilar_col2].groupby('M_PIT_RELEASE_ASSIST').nunique()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:29.496226Z","iopub.execute_input":"2022-03-04T11:40:29.496441Z","iopub.status.idle":"2022-03-04T11:40:30.189519Z","shell.execute_reply.started":"2022-03-04T11:40:29.496415Z","shell.execute_reply":"2022-03-04T11:40:30.188647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[xtremilar_col2].groupby('M_DRSASSIST').nunique()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:30.19081Z","iopub.execute_input":"2022-03-04T11:40:30.191046Z","iopub.status.idle":"2022-03-04T11:40:30.87691Z","shell.execute_reply.started":"2022-03-04T11:40:30.191017Z","shell.execute_reply":"2022-03-04T11:40:30.875974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's a litlle challenging deciding which of the 9 columns to retain, considering that none of the grouby results shows columns having values uniquely occuring once across the groups. An even more important question is whether we need any of these features to predict the targets. We will therefore implement a feature contribution assessment to evaluate which of the 9 (if any) have the highest predictive power score with respect to the targets.","metadata":{}},{"cell_type":"code","source":"_ = Dataset(df[xtremilar_col2], label=df['M_WEATHER'])\nSingleFeatureContribution().run(_)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:30.878414Z","iopub.execute_input":"2022-03-04T11:40:30.87939Z","iopub.status.idle":"2022-03-04T11:40:32.880545Z","shell.execute_reply.started":"2022-03-04T11:40:30.879345Z","shell.execute_reply":"2022-03-04T11:40:32.879711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = Dataset(df[xtremilar_col2], label=df['M_RAIN_PERCENTAGE'])\nSingleFeatureContribution().run(_)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:32.8819Z","iopub.execute_input":"2022-03-04T11:40:32.882205Z","iopub.status.idle":"2022-03-04T11:40:34.901246Z","shell.execute_reply.started":"2022-03-04T11:40:32.882162Z","shell.execute_reply":"2022-03-04T11:40:34.900408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will just drop all the 9 features. Our argument is that the AI engine doesn't take these informations into consideration to make inference on expected weather condition.","metadata":{}},{"cell_type":"code","source":"df.drop(xtremilar_col2, axis=1, inplace=True)\nprint(df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:34.90295Z","iopub.execute_input":"2022-03-04T11:40:34.903248Z","iopub.status.idle":"2022-03-04T11:40:35.123024Z","shell.execute_reply.started":"2022-03-04T11:40:34.903207Z","shell.execute_reply":"2022-03-04T11:40:35.121993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above contribution checks showed us that we do not need any of the 9 features for our model. We will apply such an assessment on the rest of our features during feature selection. For now, we want to explore the groupings in the data.","metadata":{}},{"cell_type":"markdown","source":"<br><b> Exploring the groupings in the data</b>","metadata":{}},{"cell_type":"code","source":"de = df.copy()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:35.12476Z","iopub.execute_input":"2022-03-04T11:40:35.12507Z","iopub.status.idle":"2022-03-04T11:40:35.304522Z","shell.execute_reply.started":"2022-03-04T11:40:35.125028Z","shell.execute_reply":"2022-03-04T11:40:35.3037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dd = de.groupby(['M_SESSION_UID', 'M_TIME_OFFSET'])\ndd.first()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:35.305927Z","iopub.execute_input":"2022-03-04T11:40:35.306138Z","iopub.status.idle":"2022-03-04T11:40:35.711214Z","shell.execute_reply.started":"2022-03-04T11:40:35.306113Z","shell.execute_reply":"2022-03-04T11:40:35.710361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(25, 20))\nsns.kdeplot(\n   data=df, x=\"M_WEATHER\", hue=\"M_RAIN_PERCENTAGE\",\n   fill=True, common_norm=False, palette=\"coolwarm\",\n   alpha=.5, linewidth=0,\n)\nplt.grid(False)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:35.712362Z","iopub.execute_input":"2022-03-04T11:40:35.712591Z","iopub.status.idle":"2022-03-04T11:40:44.348376Z","shell.execute_reply.started":"2022-03-04T11:40:35.712565Z","shell.execute_reply":"2022-03-04T11:40:44.347467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let us explore the distribution of some of the features within the groups of the data, and see how these features change over time, session to session.","metadata":{}},{"cell_type":"code","source":"count = 1\ncolour = ['blue','red','darkblue']\nplt.figure(figsize=(30,200))\nfor col in de.columns:\n    plt.subplot(14,2,count)\n    dd[col].mean().plot(lw=1,color=np.random.choice(colour))\n    plt.ylabel('AVERAGE_{}'.format(col)) \n    plt.title('Time Variation of {}'.format(col), fontsize = 16)\n    #plt.legend(loc='center')\n    plt.grid(False)\n    if count<36:\n        count+=1","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:44.349429Z","iopub.execute_input":"2022-03-04T11:40:44.349647Z","iopub.status.idle":"2022-03-04T11:40:51.88078Z","shell.execute_reply.started":"2022-03-04T11:40:44.349622Z","shell.execute_reply":"2022-03-04T11:40:51.87961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>As our ultimate goal is to <b> build models that learn the evolution of in-game weather conditions over time</b> features that do not show reasonable variation over time, as revealed by the plots, will be less useful for our models.\n\nBefore dropping these features, let's transform the data by regrouping by session UID, we then sort by session time and time offset. In other words, we want the data to represent the time variation, session by session.","metadata":{}},{"cell_type":"code","source":"start = de.copy()\nframes = []\nfor uid in np.sort(start['M_SESSION_UID'].unique()):\n    _ = start[start['M_SESSION_UID']==uid]\n    _.sort_values(by=['M_SESSION_TIME','M_TIME_OFFSET'], inplace=True,\n               ascending = [True, True])\n    frames.append(_)\ndtime = pd.concat(frames)\ndtime.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:51.882527Z","iopub.execute_input":"2022-03-04T11:40:51.882823Z","iopub.status.idle":"2022-03-04T11:40:53.686676Z","shell.execute_reply.started":"2022-03-04T11:40:51.882784Z","shell.execute_reply":"2022-03-04T11:40:53.685796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtime.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:53.68805Z","iopub.execute_input":"2022-03-04T11:40:53.688274Z","iopub.status.idle":"2022-03-04T11:40:53.708894Z","shell.execute_reply.started":"2022-03-04T11:40:53.688248Z","shell.execute_reply":"2022-03-04T11:40:53.707931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the preceeding plots, we explored the internal variations in the data grouped by session UID and time offset. Now that we have reordered our data, we will visualise the variations in the grouping by time offset alone and inspect what the seasonality looks like across features.","metadata":{}},{"cell_type":"code","source":"count = 1\ncolour = ['blue','red','darkblue']\nplt.figure(figsize=(30,60))\nfor col in dtime.columns.unique():\n    plt.subplot(7,4,count)\n    dtime.groupby('M_TIME_OFFSET')[col].min().plot(lw=1,color=np.random.choice(colour))\n    plt.ylabel('MIN_{}'.format(col)) \n    plt.title('Time Variation of {}'.format(col), fontsize = 16)\n    #plt.legend(loc='center')\n    plt.grid(False)\n    if count<28:\n        count+=1","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:40:53.710216Z","iopub.execute_input":"2022-03-04T11:40:53.710406Z","iopub.status.idle":"2022-03-04T11:41:01.286068Z","shell.execute_reply.started":"2022-03-04T11:40:53.710382Z","shell.execute_reply":"2022-03-04T11:41:01.285351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>The distributions would have looked different from this had it been we had plotted with the mean since this would have made it more seasonal. However, max or min are more realistic in this case as they allow us detect features that have constant max or min across time offsets. However, it is not clear from this plot whether the features that show no variation across time offsets in this plot are binary.\n\n<b>At a glance, we observe the following features are constant in time at min and max:</b><br>\n- frame ID\n- pit stop window ideal lap\n- track id\n- formula\n- session type\n- pit stop window latest lap\n- forecast accuracy\n- pit stop rejoin position\n- track temperature change\n- weather forecast sample\n- AI difficulty\n- pit speed limit\n- network game","metadata":{}},{"cell_type":"markdown","source":"Now, we will isolate these features and study their variations ***with respect to weather*** after 5, 10, 15, 30 and 60 minutes. <b>Before this, we will perform our final row reduction by removing rows with time offset 0, as there's been no indication that these provide any predictive value</b>. This is justfied by the visualisations above which show us constant observations across this time offset value.","metadata":{}},{"cell_type":"code","source":"print('Shape before dropping -->', dtime.shape)\nafterdrop = dtime.drop(dtime[dtime['M_TIME_OFFSET']==0.].index)\nprint('Shape after dropping -->', afterdrop.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:41:01.287185Z","iopub.execute_input":"2022-03-04T11:41:01.287885Z","iopub.status.idle":"2022-03-04T11:41:03.427565Z","shell.execute_reply.started":"2022-03-04T11:41:01.287849Z","shell.execute_reply":"2022-03-04T11:41:03.426666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"isolated = afterdrop[[\n            'M_SESSION_UID',\n            'M_SESSION_TYPE',\n            'M_TIME_OFFSET',\n            'M_FRAME_IDENTIFIER',\n            'M_PIT_STOP_WINDOW_IDEAL_LAP',\n            'M_TRACK_ID',\n            'M_FORMULA',\n            'M_PIT_STOP_WINDOW_LATEST_LAP',\n            'M_PIT_STOP_REJOIN_POSITION',\n            'M_TRACK_TEMPERATURE_CHANGE',\n            'M_AI_DIFFICULTY',\n            'M_PIT_SPEED_LIMIT',\n            'M_NETWORK_GAME',\n            'M_FORECAST_ACCURACY',\n            'M_WEATHER_FORECAST_SAMPLES_M_WEATHER',\n            'M_WEATHER'\n            ]]","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:41:03.428702Z","iopub.execute_input":"2022-03-04T11:41:03.428934Z","iopub.status.idle":"2022-03-04T11:41:03.466012Z","shell.execute_reply.started":"2022-03-04T11:41:03.428904Z","shell.execute_reply":"2022-03-04T11:41:03.464975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"isolated = isolated[(isolated['M_TIME_OFFSET'] <= 60) & (isolated['M_TIME_OFFSET'] > 0)]\nisolated['M_TIME_OFFSET'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:41:03.467635Z","iopub.execute_input":"2022-03-04T11:41:03.468041Z","iopub.status.idle":"2022-03-04T11:41:03.519009Z","shell.execute_reply.started":"2022-03-04T11:41:03.467992Z","shell.execute_reply":"2022-03-04T11:41:03.51817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = 1\ncolour = ['blue','red','darkblue']\nplt.figure(figsize=(30,60))\nfor col in isolated.columns:\n    plt.subplot(7,4,count)\n    isolated.groupby(['M_TIME_OFFSET','M_WEATHER'])[col].mean().plot(lw=1,color=np.random.choice(colour)) #plotting by mode, implemented by finding the highest value count\n    plt.ylabel('MEAN_{}'.format(col)) \n    plt.title('Weather Variation of {}'.format(col), fontsize = 16)\n    #plt.legend(loc='center')\n    plt.grid(False)\n    if count<16:\n        count+=1","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:41:03.520425Z","iopub.execute_input":"2022-03-04T11:41:03.520869Z","iopub.status.idle":"2022-03-04T11:41:07.19544Z","shell.execute_reply.started":"2022-03-04T11:41:03.520823Z","shell.execute_reply":"2022-03-04T11:41:07.194613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br> We are being careful at this point not to second guess dropping of unwanted features, as many of these features, despite demonstrating no variation in the time offsets, have been shown in our earliest observations to vary session ID by session ID. We will therefore leave the rest of the preprocessing to the next section on feature selection.","metadata":{}},{"cell_type":"code","source":"#print('Downloading data...')\n#afterdrop.to_csv('eda_output.csv')\n#print('Done!')","metadata":{"execution":{"iopub.status.busy":"2022-03-03T20:44:33.145009Z","iopub.execute_input":"2022-03-03T20:44:33.146002Z","iopub.status.idle":"2022-03-03T20:44:33.15042Z","shell.execute_reply.started":"2022-03-03T20:44:33.145955Z","shell.execute_reply":"2022-03-03T20:44:33.14932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<h4><b>2. Feature Selection Experimentations</b></h4>\n\nWe begin this section by obtaining a feature importance ranking. Then we perform feature selection using two algorithms side by side: first with **generic univariate select**, and second with **recursive feature elimination**. We compare the results from both procedures and immdediately eliminate those features that were not selected by both.","metadata":{}},{"cell_type":"markdown","source":"**(a) Ranking Feature Importance**\n\nWe will implement Kepler Variable Importance to rank feature importance for each of our targets:","metadata":{}},{"cell_type":"code","source":"target1 = afterdrop['M_WEATHER']\ntarget2 = afterdrop['M_RAIN_PERCENTAGE']\nfeatures1 = afterdrop.drop('M_WEATHER', axis=1)\nfeatures2 = afterdrop.drop('M_RAIN_PERCENTAGE', axis=1)\nfeatures1.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:47:20.031646Z","iopub.execute_input":"2022-03-04T11:47:20.032009Z","iopub.status.idle":"2022-03-04T11:47:20.287838Z","shell.execute_reply.started":"2022-03-04T11:47:20.031976Z","shell.execute_reply":"2022-03-04T11:47:20.286896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features2.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:47:21.371064Z","iopub.execute_input":"2022-03-04T11:47:21.371767Z","iopub.status.idle":"2022-03-04T11:47:21.37799Z","shell.execute_reply.started":"2022-03-04T11:47:21.371707Z","shell.execute_reply":"2022-03-04T11:47:21.376826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<i> **Rank feature importance for Target 1** </i>🎯 ","metadata":{}},{"cell_type":"code","source":"kepler_mutual_information = mutual_info_classif(features1, target1)\n\nplt.subplots(1, figsize=(27, 1))\nsns.heatmap(kepler_mutual_information[:, np.newaxis].T, cmap='Blues', cbar=False, linewidths=1, annot=True)\nplt.yticks([], [])\nplt.gca().set_xticklabels(features1.columns[0:], rotation=45, ha='right', fontsize=12)\nplt.suptitle(\"Kepler Variable Importance (mutual_info_classif)\", fontsize=18, y=1.2)\nplt.gcf().subplots_adjust(wspace=0.2)\npass","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:47:22.914532Z","iopub.execute_input":"2022-03-04T11:47:22.914835Z","iopub.status.idle":"2022-03-04T11:51:37.309333Z","shell.execute_reply.started":"2022-03-04T11:47:22.914805Z","shell.execute_reply":"2022-03-04T11:51:37.308582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<i> **Rank feature importance for Target 2** </i> 🎯 ","metadata":{}},{"cell_type":"code","source":"kepler_mutual_information_2 = mutual_info_classif(features2, target2)\n\nplt.subplots(1, figsize=(27, 1))\nsns.heatmap(kepler_mutual_information_2[:, np.newaxis].T, cmap='Blues', cbar=False, linewidths=1, annot=True)\nplt.yticks([], [])\nplt.gca().set_xticklabels(features2.columns[0:], rotation=45, ha='right', fontsize=12)\nplt.suptitle(\"Kepler Variable Importance (mutual_info_classif)\", fontsize=18, y=1.2)\nplt.gcf().subplots_adjust(wspace=0.2)\npass","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:51:37.311231Z","iopub.execute_input":"2022-03-04T11:51:37.311987Z","iopub.status.idle":"2022-03-04T11:55:47.367763Z","shell.execute_reply.started":"2022-03-04T11:51:37.311931Z","shell.execute_reply":"2022-03-04T11:55:47.366985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br> **(b) Feature Selection with Kepler Variable Importance (Generic Univariate Select)**","metadata":{}},{"cell_type":"code","source":"def feature_selection_table(selected, original_features):\n    \"\"\"\n    Display a table that indicates whether or not a feature was selected\n    \"\"\"\n    status = []\n    index = []\n    for col in original_features:\n        if col in selected:\n            index.append(col)\n            status.append('Selected')\n        else:\n            index.append(col)\n            status.append('Not Selected')\n            \n    _selection_table = pd.DataFrame(status, columns = ['Selection Status'], index = original_features)\n    \n    print('Number of features selected:', len(selected))\n    print('Number of features left out:', len([col for col in original_features if col not in selected]))\n    \n    return _selection_table","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:55:47.369154Z","iopub.execute_input":"2022-03-04T11:55:47.369989Z","iopub.status.idle":"2022-03-04T11:55:47.378996Z","shell.execute_reply.started":"2022-03-04T11:55:47.369949Z","shell.execute_reply":"2022-03-04T11:55:47.378323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<i> **Select features for target 1** </i>","metadata":{}},{"cell_type":"code","source":"#GenericUnivariateSelect(score_func=lambda X, y: X.mean(axis=0), mode='percentile', param=50)\ntrans = GenericUnivariateSelect(score_func=mutual_info_classif, mode = 'percentile', param=50)\nkepler_X_trans = trans.fit_transform(features1, target1)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:55:47.380607Z","iopub.execute_input":"2022-03-04T11:55:47.38142Z","iopub.status.idle":"2022-03-04T12:00:08.187775Z","shell.execute_reply.started":"2022-03-04T11:55:47.381377Z","shell.execute_reply":"2022-03-04T12:00:08.186692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_retained_Select = features1.iloc[:, :].columns[trans.get_support()].values\nfeatures_selected_1 = pd.DataFrame(kepler_X_trans, columns=columns_retained_Select)\nfeatures_selected_1.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:00:08.189093Z","iopub.execute_input":"2022-03-04T12:00:08.189334Z","iopub.status.idle":"2022-03-04T12:00:08.197711Z","shell.execute_reply.started":"2022-03-04T12:00:08.189304Z","shell.execute_reply":"2022-03-04T12:00:08.196989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Kepler_Target_1 = features_selected_1.columns\nfeature_selection_table(Kepler_Target_1, features1.columns)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:00:08.199009Z","iopub.execute_input":"2022-03-04T12:00:08.199443Z","iopub.status.idle":"2022-03-04T12:00:08.219375Z","shell.execute_reply.started":"2022-03-04T12:00:08.199401Z","shell.execute_reply":"2022-03-04T12:00:08.218713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<i> **Select features for target 2** </i>","metadata":{}},{"cell_type":"code","source":"#GenericUnivariateSelect(score_func=lambda X, y: X.mean(axis=0), mode='percentile', param=50)\ntrans_2 = GenericUnivariateSelect(score_func=mutual_info_classif, mode = 'percentile', param=50)\nkepler_X_trans_2 = trans_2.fit_transform(features2, target2)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:00:08.220891Z","iopub.execute_input":"2022-03-04T12:00:08.22129Z","iopub.status.idle":"2022-03-04T12:04:17.863508Z","shell.execute_reply.started":"2022-03-04T12:00:08.221259Z","shell.execute_reply":"2022-03-04T12:04:17.862656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_retained_Select_2 = features2.iloc[:, :].columns[trans.get_support()].values\nfeatures_selected_2 = pd.DataFrame(kepler_X_trans_2, columns=columns_retained_Select_2)\nfeatures_selected_2.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:04:17.864922Z","iopub.execute_input":"2022-03-04T12:04:17.865152Z","iopub.status.idle":"2022-03-04T12:04:17.874083Z","shell.execute_reply.started":"2022-03-04T12:04:17.865124Z","shell.execute_reply":"2022-03-04T12:04:17.873259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Kepler_Target_2 = features_selected_2.columns\nfeature_selection_table(Kepler_Target_2, features2.columns)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:04:17.875462Z","iopub.execute_input":"2022-03-04T12:04:17.875687Z","iopub.status.idle":"2022-03-04T12:04:17.896797Z","shell.execute_reply.started":"2022-03-04T12:04:17.87566Z","shell.execute_reply":"2022-03-04T12:04:17.895459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>**(c) Feature Selection by Recursive Feature Elimination (RFE)**\n\n<i> **Eliminate features for target 1** </i>\n\nSince target 1 (weather) is a binary class label, we will implement RFE using the logistic regression algorithm","metadata":{}},{"cell_type":"code","source":"estimator_1 = LogisticRegression(solver='liblinear')\nrfe_1 = RFE(estimator_1, n_features_to_select=13, step=1)\nfit_1 = rfe_1.fit(features1, target1)\nprint(\"Num Features: %s\" % (fit_1.n_features_))\nprint(\"Selected Features: %s\" % (fit_1.support_))\nprint(\"Feature Ranking: %s\" % (fit_1.ranking_))\n\n#RFE_Target_2 = fit.get_feature_names_out(features2.columns)\n#RFE_Target_2","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:04:17.899678Z","iopub.execute_input":"2022-03-04T12:04:17.899923Z","iopub.status.idle":"2022-03-04T12:04:52.243588Z","shell.execute_reply.started":"2022-03-04T12:04:17.899892Z","shell.execute_reply":"2022-03-04T12:04:52.242666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we will use the following helper functions to generate an array of the names of features that were selected corresponding to the selection index position. This array will then be used to generate the dataframe that displays selected features.","metadata":{}},{"cell_type":"code","source":"def _check_feature_names_in(estimator, input_features=None):\n    \"\"\"Get output feature names for transformation.\n    Parameters\n    ----------\n    input_features : array-like of str or None, default=None\n        Input features.\n        - If `input_features` is `None`, then `feature_names_in_` is\n            used as feature names in. If `feature_names_in_` is not defined,\n            then names are generated: `[x0, x1, ..., x(n_features_in_)]`.\n        - If `input_features` is an array-like, then `input_features` must\n            match `feature_names_in_` if `feature_names_in_` is defined.\n    Returns\n    -------\n    feature_names_in : ndarray of str\n        Feature names in.\n    \"\"\"\n\n    feature_names_in_ = getattr(estimator, \"feature_names_in_\", None)\n    n_features_in_ = getattr(estimator, \"n_features_in_\", None)\n\n    if input_features is not None:\n        input_features = np.asarray(input_features, dtype=object)\n        if feature_names_in_ is not None and not np.array_equal(\n            feature_names_in_, input_features\n        ):\n            raise ValueError(\"input_features is not equal to feature_names_in_\")\n\n        if n_features_in_ is not None and len(input_features) != n_features_in_:\n            raise ValueError(\n                \"input_features should have length equal to number of \"\n                f\"features ({n_features_in_}), got {len(input_features)}\"\n            )\n        return input_features\n\n    if feature_names_in_ is not None:\n        return feature_names_in_\n\n    # Generates feature names if `n_features_in_` is defined\n    if n_features_in_ is None:\n        raise ValueError(\"Unable to generate feature names without n_features_in_\")\n\n    return np.asarray([f\"x{i}\" for i in range(n_features_in_)], dtype=object)\n\n\ndef get_feature_names_out(estimator, input_features=None):\n    \"\"\"Mask feature names according to selected features.\n    Parameters\n    ----------\n    input_features : array-like of str or None, default=None\n        Input features.\n        - If `input_features` is `None`, then `feature_names_in_` is\n          used as feature names in. If `feature_names_in_` is not defined,\n          then names are generated: `[x0, x1, ..., x(n_features_in_)]`.\n        - If `input_features` is an array-like, then `input_features` must\n          match `feature_names_in_` if `feature_names_in_` is defined.\n    Returns\n    -------\n    feature_names_out : ndarray of str objects\n        Transformed feature names.\n    \"\"\"\n    input_features = _check_feature_names_in(estimator, input_features)\n    return input_features[estimator.get_support()]","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:04:52.245279Z","iopub.execute_input":"2022-03-04T12:04:52.245545Z","iopub.status.idle":"2022-03-04T12:04:52.258029Z","shell.execute_reply.started":"2022-03-04T12:04:52.245513Z","shell.execute_reply":"2022-03-04T12:04:52.256905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RFE_Target_1 = get_feature_names_out(fit_1, features1.columns)\nRFE_Target_1 ","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:04:52.259383Z","iopub.execute_input":"2022-03-04T12:04:52.259659Z","iopub.status.idle":"2022-03-04T12:04:52.278656Z","shell.execute_reply.started":"2022-03-04T12:04:52.259628Z","shell.execute_reply":"2022-03-04T12:04:52.277829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_selection_table(RFE_Target_1, features1.columns)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:04:52.27999Z","iopub.execute_input":"2022-03-04T12:04:52.280346Z","iopub.status.idle":"2022-03-04T12:04:52.299324Z","shell.execute_reply.started":"2022-03-04T12:04:52.280316Z","shell.execute_reply":"2022-03-04T12:04:52.298459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br><i> **Eliminate features for target 2** </i>\n\nSince target 2 (percentage rain) is a continuous target, we will implement RFE using a decision tree regressor","metadata":{}},{"cell_type":"code","source":"estimator_2 = DecisionTreeRegressor()\nrfe = RFE(estimator_2, n_features_to_select=13, step=1)\nfit = rfe.fit(features2, target2)\n\nprint(\"Num Features: %s\" % (fit.n_features_))\nprint(\"Selected Features: %s\" % (fit.support_))\nprint(\"Feature Ranking: %s\" % (fit.ranking_))\n\n#RFE_Target_2 = fit.get_feature_names_out(features2.columns)\n#RFE_Target_2","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:04:52.300607Z","iopub.execute_input":"2022-03-04T12:04:52.300942Z","iopub.status.idle":"2022-03-04T12:13:20.242277Z","shell.execute_reply.started":"2022-03-04T12:04:52.300913Z","shell.execute_reply":"2022-03-04T12:13:20.24127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RFE_Target_2 = get_feature_names_out(fit, features2.columns)\nRFE_Target_2","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:13:20.243701Z","iopub.execute_input":"2022-03-04T12:13:20.244007Z","iopub.status.idle":"2022-03-04T12:13:20.251284Z","shell.execute_reply.started":"2022-03-04T12:13:20.243976Z","shell.execute_reply":"2022-03-04T12:13:20.250372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_selection_table(RFE_Target_2, features2.columns)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:13:20.252851Z","iopub.execute_input":"2022-03-04T12:13:20.253066Z","iopub.status.idle":"2022-03-04T12:13:20.271703Z","shell.execute_reply.started":"2022-03-04T12:13:20.25304Z","shell.execute_reply":"2022-03-04T12:13:20.270729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now we will create a function that combines the results from RFE and those from Kepler's, which will allow us compare between both**","metadata":{}},{"cell_type":"code","source":"cols_classif = ['Kepler Target 1 Results', 'RFE Target 1 Results']\nresults_classif = [Kepler_Target_1, RFE_Target_1]\nfeatures_classif = [features1.columns, features1.columns]\n\ncols_reg = ['Kepler Target 2 Results', 'RFE Target 2 Results']\nresults_reg = [Kepler_Target_2, RFE_Target_2]\nfeatures_reg = [features2.columns, features2.columns]","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:13:20.272808Z","iopub.execute_input":"2022-03-04T12:13:20.273278Z","iopub.status.idle":"2022-03-04T12:13:20.282965Z","shell.execute_reply.started":"2022-03-04T12:13:20.273244Z","shell.execute_reply":"2022-03-04T12:13:20.281957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def new_selection_table(selection_lists, features_list, cols):\n    \"\"\"\n    Display a table combining all the feature selection results for the two targets\n    \"\"\"\n    #cols = []\n    #for j in range(len(selection_lists)):\n    #    cols.append([i for i, a in locals().items() if a == '{}'.format(selection_lists[j])])\n    \n    tables_list = [feature_selection_table(i,j) for i,j in zip(selection_lists, features_list)]\n    _selection_table = pd.DataFrame(columns = cols, index = tables_list[0].index)\n    \n    for table, algorithm in zip(tables_list, cols):\n        _selection_table[algorithm] = table\n    \n    return _selection_table","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:13:20.284238Z","iopub.execute_input":"2022-03-04T12:13:20.284452Z","iopub.status.idle":"2022-03-04T12:13:20.299444Z","shell.execute_reply.started":"2022-03-04T12:13:20.284425Z","shell.execute_reply":"2022-03-04T12:13:20.298764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selection_results_classif = new_selection_table(results_classif, features_classif, cols_classif)\nselection_results_classif","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:13:20.300969Z","iopub.execute_input":"2022-03-04T12:13:20.301189Z","iopub.status.idle":"2022-03-04T12:13:20.327661Z","shell.execute_reply.started":"2022-03-04T12:13:20.301163Z","shell.execute_reply":"2022-03-04T12:13:20.326777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selection_results_reg = new_selection_table(results_reg, features_reg, cols_reg)\nselection_results_reg","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:13:20.328883Z","iopub.execute_input":"2022-03-04T12:13:20.329094Z","iopub.status.idle":"2022-03-04T12:13:20.34747Z","shell.execute_reply.started":"2022-03-04T12:13:20.329068Z","shell.execute_reply":"2022-03-04T12:13:20.346558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see a few differences in selection decisions made by the feature selection algorithms, so our next step is to immediately drop features that were eliminated both by RFE and Kepler. We will retain features that were slected by both, and decide how to handle the mixed results.","metadata":{}},{"cell_type":"code","source":"to_drop_classif = selection_results_classif[(selection_results_classif['Kepler Target 1 Results'] == 'Not Selected') \n                                                         & (selection_results_classif['RFE Target 1 Results'] == 'Not Selected')]\nto_drop_classif","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:13:20.349285Z","iopub.execute_input":"2022-03-04T12:13:20.349591Z","iopub.status.idle":"2022-03-04T12:13:20.364083Z","shell.execute_reply.started":"2022-03-04T12:13:20.349549Z","shell.execute_reply":"2022-03-04T12:13:20.363473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_drop_reg = selection_results_reg[(selection_results_reg['Kepler Target 2 Results'] == 'Not Selected') \n                                                         & (selection_results_reg['RFE Target 2 Results'] == 'Not Selected')]\nto_drop_reg","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:13:20.365642Z","iopub.execute_input":"2022-03-04T12:13:20.365925Z","iopub.status.idle":"2022-03-04T12:13:20.386005Z","shell.execute_reply.started":"2022-03-04T12:13:20.365842Z","shell.execute_reply":"2022-03-04T12:13:20.385115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<i>It turns out from the above that not all features relevant for weather classification are relevant for percentage rain prediction</i>","metadata":{}},{"cell_type":"code","source":"train1 = selection_results_classif.drop(to_drop_classif.index)\ntrain1","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:13:20.387382Z","iopub.execute_input":"2022-03-04T12:13:20.388418Z","iopub.status.idle":"2022-03-04T12:13:20.400959Z","shell.execute_reply.started":"2022-03-04T12:13:20.38837Z","shell.execute_reply":"2022-03-04T12:13:20.399813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train2 = selection_results_reg.drop(to_drop_reg.index)\ntrain2","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:13:20.402429Z","iopub.execute_input":"2022-03-04T12:13:20.402783Z","iopub.status.idle":"2022-03-04T12:13:20.419164Z","shell.execute_reply.started":"2022-03-04T12:13:20.402738Z","shell.execute_reply":"2022-03-04T12:13:20.418371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**(d) Final Feature Selection by Intuition**\n\nAfter the last series of steps, we see that the rest of the features we are left with have been either selected by both algorithms or by only one algorithm. At this point, we will now use our intuition to pick out the features we deem most relevant, based on our understanding of the data and the game so far.\n\nOur argument is that, while the features retained have shown certain levels of importance based on our earlier steps, the ones relevant for our model are simply those that will cause definitive changes to the observed outcomes (weather, %rain) if they were changed. As such, we will retain features that: (i) capture information that a player could readily provide if asked, and (ii) show potential to change the outcomes if they were varied.\n\n***The features we consider to match both criteria, supported by the preceding EDA, are:***\n- Session Time\n- Session Time Spent\n- Track Length\n- Track Temperature\n- Air Temperature\n- AI Difficulty\n- Total Laps\n\nThese along with Time Offset, Weather and Rain Percentage will make up our final dataset. We are excluding session UID because we consider that its major role was to help us organise portions of the data according to their session group.","metadata":{}},{"cell_type":"code","source":"final_data_weather = afterdrop[[\n    'M_SESSION_TIME',\n    'M_SESSION_TIME_SPENT',\n    'M_TRACK_LENGTH',\n    'M_TRACK_TEMPERATURE',\n    'M_AIR_TEMPERATURE',\n    'M_AI_DIFFICULTY',\n    'M_TOTAL_LAPS',\n    'M_TIME_OFFSET',\n    'M_WEATHER',\n    'M_RAIN_PERCENTAGE'\n    \n]]\n\nprint(final_data_weather.shape)\nfinal_data_weather.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:13:20.420532Z","iopub.execute_input":"2022-03-04T12:13:20.421181Z","iopub.status.idle":"2022-03-04T12:13:20.453514Z","shell.execute_reply.started":"2022-03-04T12:13:20.421142Z","shell.execute_reply":"2022-03-04T12:13:20.452639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have ended up with 8 input features, we will validate our choice using the following plots that show the ***variation of weather against each input feature grouped by time offsets***. This is the reverse of what we did earlier where we showed the distribution of each input feature with respect to weather and time offsets. We will repeat the same procedure for rain percentage as the dependent variable.","metadata":{}},{"cell_type":"code","source":"count = 1\ncolour = ['blue','darkred','darkblue']\nplt.figure(figsize=(30,60))\nfor col in final_data_weather.columns:\n    plt.subplot(5,2,count)\n    final_data_weather.groupby(['M_TIME_OFFSET',col])['M_WEATHER'].mean().plot(lw=1,color=np.random.choice(colour))\n    plt.ylabel('AVERAGE WEATHER') \n    plt.title('VARIATION OF WEATHER WITH {}'.format(col), fontsize = 16)\n    plt.grid(False)\n    if count<12:\n        count+=1","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:13:20.454976Z","iopub.execute_input":"2022-03-04T12:13:20.455873Z","iopub.status.idle":"2022-03-04T12:13:26.618984Z","shell.execute_reply.started":"2022-03-04T12:13:20.455828Z","shell.execute_reply":"2022-03-04T12:13:26.618192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = 1\ncolour = ['blue','darkred','darkblue']\nplt.figure(figsize=(30,60))\nfor col in final_data_weather.columns:\n    plt.subplot(5,2,count)\n    final_data_weather.groupby(['M_TIME_OFFSET',col])['M_RAIN_PERCENTAGE'].mean().plot(lw=1,color=np.random.choice(colour))\n    plt.ylabel('AVERAGE RAIN PERCENTAGE') \n    plt.title('VARIATION OF RAIN PERCENTAGE WITH {}'.format(col), fontsize = 16)\n    plt.grid(False)\n    if count<12:\n        count+=1","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:13:26.620182Z","iopub.execute_input":"2022-03-04T12:13:26.620606Z","iopub.status.idle":"2022-03-04T12:13:30.153668Z","shell.execute_reply.started":"2022-03-04T12:13:26.620567Z","shell.execute_reply":"2022-03-04T12:13:30.152744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#final_data_weather.reset_index(drop=True, inplace=True)\nfinal_data_weather.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:13:30.156696Z","iopub.execute_input":"2022-03-04T12:13:30.157244Z","iopub.status.idle":"2022-03-04T12:13:30.169839Z","shell.execute_reply.started":"2022-03-04T12:13:30.157206Z","shell.execute_reply":"2022-03-04T12:13:30.16892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#final_data_weather.to_csv('./final_data_weather.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T04:36:36.337873Z","iopub.execute_input":"2022-03-04T04:36:36.338144Z","iopub.status.idle":"2022-03-04T04:36:39.553176Z","shell.execute_reply.started":"2022-03-04T04:36:36.338112Z","shell.execute_reply":"2022-03-04T04:36:39.552497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ","metadata":{}},{"cell_type":"markdown","source":"## Part III: Modelling Methodology\n<h4><b>Overview</b></h4>\n\nThis is the Machine Learning component of our solution to the FormulaAI Hack 2022 Competition. The workflow for this notebook is outlined as follows:\n- Cross-Validation Setup\n- Experiments with Feature Transformation\n- Model Experimentations I: Weather Classification\n- Classification Leaderboard Ranking\n- Model Experimentations II: Regression\n- Regression Leaderboard Ranking \n- Holdout Evaluation","metadata":{}},{"cell_type":"code","source":"import os\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', None, 'display.max_rows', 100)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\n#import deepchecks as dc\n\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\n\nfrom xgboost import XGBClassifier\nfrom xgboost import XGBRFClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n#from catboost import CatBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRFRegressor\nfrom sklearn.svm import SVR\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import RandomizedSearchCV\n#from sklearn.grid_search import HalvingGridSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.linear_model import ElasticNet\n\nimport random\nimport time\nfrom datetime import datetime","metadata":{"execution":{"iopub.status.busy":"2022-03-04T09:48:32.430418Z","iopub.execute_input":"2022-03-04T09:48:32.430792Z","iopub.status.idle":"2022-03-04T09:48:35.117558Z","shell.execute_reply.started":"2022-03-04T09:48:32.4307Z","shell.execute_reply":"2022-03-04T09:48:35.116572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Read the data***","metadata":{}},{"cell_type":"code","source":"#final_data_weather = pd.read_csv('./final_data_weather.csv', index_col = False)\nprint(final_data_weather.shape)\nfinal_data_weather.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T09:54:06.012909Z","iopub.execute_input":"2022-03-04T09:54:06.013501Z","iopub.status.idle":"2022-03-04T09:54:06.805863Z","shell.execute_reply.started":"2022-03-04T09:54:06.013435Z","shell.execute_reply":"2022-03-04T09:54:06.80501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weather_X = final_data_weather.drop('M_WEATHER', axis=1)\nweather_y = final_data_weather['M_WEATHER']\n\nrain_X = final_data_weather.drop('M_RAIN_PERCENTAGE', axis=1)\nrain_y = final_data_weather['M_RAIN_PERCENTAGE']\n\nprint(weather_X.shape)\nprint(rain_X.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T09:54:19.179273Z","iopub.execute_input":"2022-03-04T09:54:19.179862Z","iopub.status.idle":"2022-03-04T09:54:19.223132Z","shell.execute_reply.started":"2022-03-04T09:54:19.17982Z","shell.execute_reply":"2022-03-04T09:54:19.222261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<h4><b>1. Cross-Validation Set Up</b></h4>","metadata":{}},{"cell_type":"markdown","source":"***Creating train, test and validation sets***\n    \nWe first split our data into train and test sets. The test set is our holdout set and will not be unlocked until the end of each of the 2 sequences of experiments for classification and regression, respectively. The validation set will be split out of the train data and will be used for primary evaluation and to compute cross validation scores in each of our experiments.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(weather_X, weather_y, test_size=.25, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T09:54:23.233693Z","iopub.execute_input":"2022-03-04T09:54:23.233958Z","iopub.status.idle":"2022-03-04T09:54:23.346188Z","shell.execute_reply.started":"2022-03-04T09:54:23.233929Z","shell.execute_reply":"2022-03-04T09:54:23.345125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Considering the huge class imbalance in the weather target, ***we will implement repeated k-fold cross validation to further split our train data***. For our classification experiments, we will use ***repeated stratified k-fold cross validation***. We choose the value of 10 for *k* as this value has been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance. In other words, we are choosing *k = 10* to achieve reasonable bias-variance trade-off in training.","metadata":{}},{"cell_type":"code","source":"#CV configuration for classificatoiclassification \nskfold = RepeatedStratifiedKFold(n_splits=10, n_repeats = 2, random_state=1)\n\n#CV configuration for regression\nkfold = RepeatedKFold(n_splits=10, n_repeats = 2, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T09:54:24.299214Z","iopub.execute_input":"2022-03-04T09:54:24.299998Z","iopub.status.idle":"2022-03-04T09:54:24.304966Z","shell.execute_reply.started":"2022-03-04T09:54:24.299955Z","shell.execute_reply":"2022-03-04T09:54:24.303841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we will define a helper function that we will use for all our experiments. This function will also help to return validation scores for each experiment. We will customise this function a bit when we get to regression.","metadata":{}},{"cell_type":"code","source":"def training(X_train, y_train, model):\n\n    fold_no = 1    \n    n_scores, log_scores = [],[]\n    for train_index, val_index in skfold.split(X_train, y_train):\n        # select rows\n        train_X, val_X = X_train.iloc[train_index], X_train.iloc[val_index]\n        train_y, val_y = y_train.iloc[train_index], y_train.iloc[val_index]\n        \n        model.fit(train_X, train_y)\n        \n        n_scores.append(model.score(val_X, val_y))\n        log_scores.append(log_loss(val_y, model.predict_proba(val_X), labels = [0,1,2]))\n        print('For Fold {}, the Accuracy is {},'.format(str(fold_no), n_scores[fold_no - 1]), \n              'and the LogLoss is', log_scores[fold_no - 1])\n\n        fold_no += 1\n     \n    #cv_score = cross_val_score(model, train_X, train_y, scoring='accuracy', cv=skfold, n_jobs=-1, error_score='raise')\n    mean_accuracy, std_accuracy = np.mean(n_scores), np.std(n_scores)\n    mean_loss, std_loss = np.mean(log_scores), np.std(log_scores)\n    \n    print('\\n======================================')\n    \n    print('Mean Accuracy: %.3f (%.3f)' % (mean_accuracy, std_accuracy))\n    #print('Mean CV Accuracy: %.3f (%.3f)' % (np.mean(cv_score), np.std(cv_score)))\n    #\n    #print('\\n======================================')\n    #\n    print('Average LogLoss: {} ({})'.format(mean_loss, std_loss))\n\n    return model, mean_loss","metadata":{"execution":{"iopub.status.busy":"2022-03-04T09:54:26.739624Z","iopub.execute_input":"2022-03-04T09:54:26.73988Z","iopub.status.idle":"2022-03-04T09:54:26.750442Z","shell.execute_reply.started":"2022-03-04T09:54:26.739852Z","shell.execute_reply":"2022-03-04T09:54:26.749565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<h4><b>2. Experiments with Feature Transformation</b></h4>\n\nFor each algorithm we experiment with, we wil implement a variation using a scaled version of the training features. The following plots show the individual distribution of each input feature.","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(5,2, figsize = (30,60))\naxx = ax.flatten()\nfor index, col in enumerate(final_data_weather.columns):\n#     plt.figure(figsize=(15,8))\n    sns.histplot(final_data_weather[col], ax=axx[index])","metadata":{"execution":{"iopub.status.busy":"2022-03-04T09:54:40.140397Z","iopub.execute_input":"2022-03-04T09:54:40.140719Z","iopub.status.idle":"2022-03-04T09:54:50.856377Z","shell.execute_reply.started":"2022-03-04T09:54:40.140686Z","shell.execute_reply":"2022-03-04T09:54:50.855511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the distribtion of the features, we see that it may be realistic to standardise the data given the absence of normality across features. We will run a variation of each of our modelling experiment with MinMaxScalar and narrow down the range of input values to [0, 10]. \n\nWe wish to also test variations with RobustScaler to scale down input features in such a way that the influence of huge marginal points in the data are cancelled out.","metadata":{}},{"cell_type":"markdown","source":"***During these experiments with the scaled version of the data, we will make use of pipelines to avoid information leakage from our test data into the models we want to train.***\n\nWe define a pipeline construct below that implements the desired scaling on the training input features, then fits a model on this data. We will experiment with various models with and without feature transformation and see how they perform.","metadata":{}},{"cell_type":"code","source":"def scale_pipe(model_name, transformer, x_train, y_train):\n    \n    pipe = Pipeline([('scaler', transformer), ('model', model_name)])\n    model_pipeline, mean_loss = training(x_train, y_train, pipe)\n    \n    return model_pipeline, mean_loss","metadata":{"execution":{"iopub.status.busy":"2022-03-04T09:54:50.857973Z","iopub.execute_input":"2022-03-04T09:54:50.85819Z","iopub.status.idle":"2022-03-04T09:54:50.863507Z","shell.execute_reply.started":"2022-03-04T09:54:50.858162Z","shell.execute_reply":"2022-03-04T09:54:50.862606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<h4><b>3. Model Experimentations I: Weather Classification</b></h4>\nHere are the 4 classification algorithms we will experiment with:\n\n- XGBoost Classifier\n- XGB Random Forest Classifier\n- Light Gradient Boosted Machines Classifier\n- Gradient Boosted Trees with CatBoost","metadata":{}},{"cell_type":"markdown","source":"##### **(a) XGBoost Classifier**","metadata":{}},{"cell_type":"markdown","source":"*Experiment 1: Without Standardisation*","metadata":{}},{"cell_type":"code","source":"xgb = XGBClassifier()\nmodel_xgb, model_xgb_loss = training(X_train, y_train, xgb)\nmodel_xgb","metadata":{"execution":{"iopub.status.busy":"2022-03-04T09:54:50.864796Z","iopub.execute_input":"2022-03-04T09:54:50.864996Z","iopub.status.idle":"2022-03-04T10:05:39.21031Z","shell.execute_reply.started":"2022-03-04T09:54:50.864971Z","shell.execute_reply":"2022-03-04T10:05:39.20956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_xgb_loss","metadata":{"execution":{"iopub.status.busy":"2022-03-04T10:05:39.212346Z","iopub.execute_input":"2022-03-04T10:05:39.219613Z","iopub.status.idle":"2022-03-04T10:05:39.226816Z","shell.execute_reply.started":"2022-03-04T10:05:39.219545Z","shell.execute_reply":"2022-03-04T10:05:39.22599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Experiment 2: With Standardisation - MinMax and Robust","metadata":{}},{"cell_type":"code","source":"#MinMaxScaler\nxgb2 = XGBClassifier()\ntransformer = MinMaxScaler(feature_range=(0, 10),copy=False)\nxgb_scaled, xgb_scaled_loss = scale_pipe(xgb2, transformer, X_train, y_train)\nxgb_scaled_loss","metadata":{"execution":{"iopub.status.busy":"2022-03-04T10:05:39.22823Z","iopub.execute_input":"2022-03-04T10:05:39.228694Z","iopub.status.idle":"2022-03-04T10:16:36.5144Z","shell.execute_reply.started":"2022-03-04T10:05:39.228653Z","shell.execute_reply":"2022-03-04T10:16:36.513457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#RobustScaler\nxgb3 = XGBClassifier()\ntransformer = RobustScaler()\nxgb_scaled_2, xgb_scaled_2_loss = scale_pipe(xgb3, transformer, X_train, y_train)\nxgb_scaled_2_loss","metadata":{"execution":{"iopub.status.busy":"2022-03-04T10:16:36.515838Z","iopub.execute_input":"2022-03-04T10:16:36.51652Z","iopub.status.idle":"2022-03-04T10:27:29.689646Z","shell.execute_reply.started":"2022-03-04T10:16:36.516474Z","shell.execute_reply":"2022-03-04T10:27:29.688749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### **(b) XGBoost Random Forest Classifier**\nExperiment 1: Without Standardisation","metadata":{}},{"cell_type":"code","source":"xgbrf = XGBRFClassifier(n_estimators=10, subsample=0.9, colsample_bynode=0.2)\nxgb_forest, xgb_forest_loss = training(X_train, y_train, xgbrf)\nxgb_forest","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:21:57.965854Z","iopub.execute_input":"2022-03-04T12:21:57.966161Z","iopub.status.idle":"2022-03-04T12:22:40.867077Z","shell.execute_reply.started":"2022-03-04T12:21:57.966132Z","shell.execute_reply":"2022-03-04T12:22:40.866191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Experiment 2: With Standardisation","metadata":{}},{"cell_type":"code","source":"#MinMaxScaler\n\nxgbrf2 = XGBRFClassifier(n_estimators=20, subsample=0.9, colsample_bynode=0.2)\n\ntransformer = MinMaxScaler(feature_range=(0, 10),copy=False)\nxgb_forest_scaled, xgb_forest_scaled_loss = scale_pipe(xgbrf2, transformer, X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:22:40.868785Z","iopub.execute_input":"2022-03-04T12:22:40.869022Z","iopub.status.idle":"2022-03-04T12:24:04.913719Z","shell.execute_reply.started":"2022-03-04T12:22:40.868995Z","shell.execute_reply":"2022-03-04T12:24:04.91293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#RobustScaler\n\nxgbrf3 = XGBRFClassifier(n_estimators=20, subsample=0.9, colsample_bynode=0.2)\n\ntransformer = RobustScaler()\nxgb_forest_scaled_2, xgb_forest_scaled_2_loss = scale_pipe(xgbrf3, transformer, X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:24:04.917757Z","iopub.execute_input":"2022-03-04T12:24:04.918637Z","iopub.status.idle":"2022-03-04T12:25:28.938162Z","shell.execute_reply.started":"2022-03-04T12:24:04.918576Z","shell.execute_reply":"2022-03-04T12:25:28.937505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### **(c) Light Gradient Boosted Machines Classifier**\n\n*First Experiment: Without Standardisation*","metadata":{}},{"cell_type":"code","source":"lgb = LGBMClassifier()\nlgb_model, lgb_loss = training(X_train, y_train, lgb)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:25:28.940293Z","iopub.execute_input":"2022-03-04T12:25:28.942535Z","iopub.status.idle":"2022-03-04T12:26:55.458457Z","shell.execute_reply.started":"2022-03-04T12:25:28.942458Z","shell.execute_reply":"2022-03-04T12:26:55.457725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Second Experiment: With Standardisation*","metadata":{}},{"cell_type":"code","source":"#MinMaxScaler\n\nlgb2 = LGBMClassifier()\ntransformer = MinMaxScaler(feature_range=(0, 10),copy=False)\nlgb_scaled, lgb_scaled_loss = scale_pipe(lgb2, transformer, X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:26:55.462033Z","iopub.execute_input":"2022-03-04T12:26:55.463972Z","iopub.status.idle":"2022-03-04T12:28:24.286939Z","shell.execute_reply.started":"2022-03-04T12:26:55.463922Z","shell.execute_reply":"2022-03-04T12:28:24.286167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#RobustScaler\n\nlgb3 = LGBMClassifier()\ntransformer = RobustScaler()\nlgb_scaled_2, lgb_scaled_2_loss = scale_pipe(lgb3, transformer, X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:28:24.29059Z","iopub.execute_input":"2022-03-04T12:28:24.292368Z","iopub.status.idle":"2022-03-04T12:30:02.33967Z","shell.execute_reply.started":"2022-03-04T12:28:24.292333Z","shell.execute_reply":"2022-03-04T12:30:02.338838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### **(d) Gradient Boosted Trees with CatBoost Classifier**\n\n*First Experiment: Without Standardisation*","metadata":{}},{"cell_type":"code","source":"ctb = CatBoostClassifier(verbose=0, n_estimators=100)\nctb_model, ctb_loss = training(X_train, y_train, ctb)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:30:02.343432Z","iopub.execute_input":"2022-03-04T12:30:02.343737Z","iopub.status.idle":"2022-03-04T12:32:50.036157Z","shell.execute_reply.started":"2022-03-04T12:30:02.343704Z","shell.execute_reply":"2022-03-04T12:32:50.034993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Experiment 2: With Standardisation*","metadata":{}},{"cell_type":"code","source":"#MinMaxScaler\n\nctb2 = CatBoostClassifier(verbose=0, n_estimators=100)\ntransformer = MinMaxScaler(feature_range=(0, 10), copy=False)\n\nctb_scaled, ctb_scaled_loss = scale_pipe(ctb2, transformer, X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:32:50.038837Z","iopub.execute_input":"2022-03-04T12:32:50.039233Z","iopub.status.idle":"2022-03-04T12:35:32.42306Z","shell.execute_reply.started":"2022-03-04T12:32:50.039184Z","shell.execute_reply":"2022-03-04T12:35:32.422245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#RobustScaler\n\nctb3 = CatBoostClassifier(verbose=0, n_estimators=100)\ntransformer = RobustScaler()\n\nctb_scaled_2, ctb_scaled_2_loss = scale_pipe(ctb3, transformer, X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:35:32.424608Z","iopub.execute_input":"2022-03-04T12:35:32.424838Z","iopub.status.idle":"2022-03-04T12:38:16.676911Z","shell.execute_reply.started":"2022-03-04T12:35:32.42481Z","shell.execute_reply":"2022-03-04T12:38:16.676016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<h4><b>4. Classification Leaderboard</b></h4>\n\nBefore we unleash our holdout set which is 25% of our entire dataset, we will create a leaderboard of models built so far, to rank them by their mean logloss during cross-validation.","metadata":{}},{"cell_type":"code","source":"unpiped_models = [model_xgb_loss, xgb_forest_loss, lgb_loss, ctb_loss]\npiped_models = [xgb_scaled_loss, xgb_forest_scaled_loss, lgb_scaled_loss, ctb_scaled_loss]\npiped_models_2 = [xgb_scaled_2_loss, xgb_forest_scaled_2_loss, lgb_scaled_2_loss, ctb_scaled_2_loss]\nmodel_index = ['XGBoost','XGBoost Random Forest','LightGBM', 'CatBoost Classifier']\n\npre_leaderboard_1 = pd.DataFrame({'Log Loss (Without Standardisation)': unpiped_models, \n                                  'Log Loss (With MinMaxScaler)': piped_models,\n                                  'Log Loss (With RobustScaler)': piped_models_2}, index = model_index)\n\npd.set_option('display.float_format', lambda x: '%.15f' % x)\npre_leaderboard_1","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:38:16.680703Z","iopub.execute_input":"2022-03-04T12:38:16.680951Z","iopub.status.idle":"2022-03-04T12:38:16.697027Z","shell.execute_reply.started":"2022-03-04T12:38:16.68092Z","shell.execute_reply":"2022-03-04T12:38:16.696381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's set the tabe in ascending order, starting with the first column. We are ranking in ascending order as the goal of the learning algorithms is to minimise the log loss.","metadata":{}},{"cell_type":"code","source":"pre_leaderboard_1.nsmallest(4, 'Log Loss (Without Standardisation)')","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:38:16.69822Z","iopub.execute_input":"2022-03-04T12:38:16.69846Z","iopub.status.idle":"2022-03-04T12:38:16.713389Z","shell.execute_reply.started":"2022-03-04T12:38:16.69843Z","shell.execute_reply":"2022-03-04T12:38:16.712432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre_leaderboard_1.nsmallest(4, 'Log Loss (With MinMaxScaler)')","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:38:16.714873Z","iopub.execute_input":"2022-03-04T12:38:16.715135Z","iopub.status.idle":"2022-03-04T12:38:16.727387Z","shell.execute_reply.started":"2022-03-04T12:38:16.715105Z","shell.execute_reply":"2022-03-04T12:38:16.726354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre_leaderboard_1.nsmallest(4, 'Log Loss (With RobustScaler)')","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:38:16.728995Z","iopub.execute_input":"2022-03-04T12:38:16.729503Z","iopub.status.idle":"2022-03-04T12:38:16.74283Z","shell.execute_reply.started":"2022-03-04T12:38:16.729438Z","shell.execute_reply":"2022-03-04T12:38:16.74171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Key insights from the Classification Leaderboard:***\n\n- XGBoost performed nearly the same with and without standardisation. However, it performed the best with RobustScaler.\n- CatBoost performed the same  with and without standardisation\n- XGBoost Random Forest performed better with standardisation, in particular with MinMax.\n- In all cases, XGBoost outperformed other models by a large margin. \n- In all cases, XGB Random Forest came behind other models by a large order.\n\nWe will therefore select XGBoost with RobustScaler when evaluating with our test (holdout) set.","metadata":{}},{"cell_type":"markdown","source":"<br>\n<h4><b>5. Model Experimentations II: Regression</b></h4>\nHere are the 4 regression algorithms we will experiment with:\n\n- RandomForest Regressor\n- Light Gradient Boosted Trees Regressor (with Early Stopping)\n- ElasticNet Regularised Regression with Grid Search Optimisation\n- XGBoost Regressor\n\nBefore we proceed, we specify a new train-test split using the rain percentage data. We also change our k-fold algorithm to RepeatedKFold. In each of these experiments, we will use cross validation to give us insights into which model performs the best before we then unlock holdout to apply the best algorithm.","metadata":{}},{"cell_type":"code","source":"X1_train, X1_test, y1_train, y1_test = train_test_split(rain_X, rain_y, test_size=.25, random_state=42)\nkfold = RepeatedKFold(n_splits=10, n_repeats = 2, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:48:48.976206Z","iopub.execute_input":"2022-03-04T12:48:48.976615Z","iopub.status.idle":"2022-03-04T12:48:49.07864Z","shell.execute_reply.started":"2022-03-04T12:48:48.976574Z","shell.execute_reply":"2022-03-04T12:48:49.077554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reg_train(X1_train, y1_train, model):\n\n    fold_no = 1    \n    n_scores, mae_scores = [],[]\n    for train_index, val_index in kfold.split(X1_train, y1_train):\n        train_X, val_X = X1_train.iloc[train_index], X1_train.iloc[val_index]\n        train_y, val_y = y1_train.iloc[train_index], y1_train.iloc[val_index]\n        \n        model.fit(train_X, train_y)\n        \n        n_scores.append(model.score(val_X, val_y))\n        mae_scores.append(mean_absolute_error(val_y, model.predict(val_X)))\n        print('For Fold {}, the Accuracy is {},'.format(str(fold_no), n_scores[fold_no - 1]), \n              'and the LogLoss is', mae_scores[fold_no - 1])\n        \n        fold_no += 1\n     \n    mean_accuracy, std_accuracy = np.mean(n_scores), np.std(n_scores)\n    mean_mae, std_mae = np.mean(mae_scores), np.std(mae_scores)\n    \n    print('\\n======================================')\n    \n    print('Average Accuracy Score: {} ({})'.format(mean_accuracy, std_accuracy))\n    print('Average MAE and Standard Deviation: {} ({})'.format(mean_mae, std_mae))\n\n    return model, mean_mae","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:48:49.56915Z","iopub.execute_input":"2022-03-04T12:48:49.569442Z","iopub.status.idle":"2022-03-04T12:48:49.580219Z","shell.execute_reply.started":"2022-03-04T12:48:49.569413Z","shell.execute_reply":"2022-03-04T12:48:49.579591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***We also define a new pipeline construct that incorporates the cross-validation process.***","metadata":{"execution":{"iopub.status.busy":"2022-03-03T15:15:45.689114Z","iopub.execute_input":"2022-03-03T15:15:45.690911Z","iopub.status.idle":"2022-03-03T15:15:45.699379Z","shell.execute_reply.started":"2022-03-03T15:15:45.690843Z","shell.execute_reply":"2022-03-03T15:15:45.69795Z"}}},{"cell_type":"code","source":"def reg_pipe(model_name, transformer, x_train, y_train):\n    \n    pipe = Pipeline([('scaler', transformer), ('cv', model_name)])\n    scores = cross_val_score(pipe, x_train, y_train, scoring='neg_mean_absolute_error', \n                            cv=kfold, n_jobs=-1, error_score='raise')\n    \n    scaled_mae = np.mean(scores)\n    \n    return pipe, scaled_mae","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:48:53.774089Z","iopub.execute_input":"2022-03-04T12:48:53.77486Z","iopub.status.idle":"2022-03-04T12:48:53.780308Z","shell.execute_reply.started":"2022-03-04T12:48:53.774816Z","shell.execute_reply":"2022-03-04T12:48:53.779687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### **(a) Random Forest Regressor**\n\n*First Experiment: Without Standardisation*","metadata":{}},{"cell_type":"code","source":"rforest = RandomForestRegressor(n_estimators=20)\nrforest, rforest_mae = reg_train(X1_train, y1_train, rforest)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:49:14.600301Z","iopub.execute_input":"2022-03-04T12:49:14.600612Z","iopub.status.idle":"2022-03-04T12:56:42.412149Z","shell.execute_reply.started":"2022-03-04T12:49:14.600582Z","shell.execute_reply":"2022-03-04T12:56:42.411131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Second Experiment: With Standardisation*","metadata":{}},{"cell_type":"code","source":"r_forest_scaled, r_forest_scaled_mae = reg_pipe(RandomForestRegressor(n_estimators = 20), MinMaxScaler(feature_range=(0,10)), X1_train, y1_train)\nr_forest_scaled_mae","metadata":{"execution":{"iopub.status.busy":"2022-03-04T12:57:39.417441Z","iopub.execute_input":"2022-03-04T12:57:39.418284Z","iopub.status.idle":"2022-03-04T13:00:51.673585Z","shell.execute_reply.started":"2022-03-04T12:57:39.418237Z","shell.execute_reply":"2022-03-04T13:00:51.672832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r_forest_scaled_2, r_forest_scaled_2_mae = reg_pipe(RandomForestRegressor(n_estimators = 20), RobustScaler(), X1_train, y1_train)\nr_forest_scaled_2_mae","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:00:51.67554Z","iopub.execute_input":"2022-03-04T13:00:51.676315Z","iopub.status.idle":"2022-03-04T13:03:58.735894Z","shell.execute_reply.started":"2022-03-04T13:00:51.676279Z","shell.execute_reply":"2022-03-04T13:03:58.735195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### **(b) Light Gradient Boosted Trees Regressor**\n\n*First Experiment: Without Standardisation (and with Early Stopping)*","metadata":{}},{"cell_type":"code","source":"lgb = LGBMRegressor()\nlgbr, lgb_mae = reg_train(X1_train, y1_train, lgb)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:03:58.737102Z","iopub.execute_input":"2022-03-04T13:03:58.737799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Second Experiment: With Standardisation (and Without Early Stopping)*","metadata":{}},{"cell_type":"code","source":"lgb_scaled, lgb_scaled_mae = reg_pipe(LGBMRegressor(), MinMaxScaler(feature_range=(0,10)), X1_train, y1_train)\nlgb_scaled_mae","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_scaled_2, lgb_scaled_2_mae = reg_pipe(LGBMRegressor(), RobustScaler(), X1_train, y1_train)\nlgb_scaled_2_mae","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### **(c) ElasticNet Regularised Regression with GridSearch Optimisation**","metadata":{}},{"cell_type":"markdown","source":"We will implement this algorithm by tuning both the ***L1 and L2*** penalties during training. A grid search is performed to find the ***alpha*** hyperparameter value that assigns the best weights to the contributions of the *L1* and *L2* penalties. The emerging alpha and L1 ratio are then used to fit the ElasticNet model.","metadata":{}},{"cell_type":"code","source":"ratios = np.arange(0, 1, 0.01)\nalphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]\n\ncv = RepeatedKFold(n_splits = 5, n_repeats=1, random_state=1)\n\nstart_model = ElasticNetCV(l1_ratio = ratios, alphas = alphas, cv = cv, n_jobs = -1)\n# fit model\nstart_model.fit(X1_train, y1_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(start_model.alpha_)\nprint(start_model.l1_ratio_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Implement cross-validation for ElasticNet across train data\n\nenet = ElasticNet(alpha = start_model.alpha_,\n                  l1_ratio = start_model.l1_ratio_)\n\nenet_model, enet_mae = reg_train(X1_train, y1_train, enet)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Second Experiment: With Standardisation*","metadata":{}},{"cell_type":"code","source":"enet_scaled = ElasticNet(alpha = start_model.alpha_,\n                        l1_ratio = start_model.l1_ratio_)\n\nenet_scaled, enet_scaled_mae = reg_pipe(enet_scaled, MinMaxScaler(feature_range=(0,10)), X1_train, y1_train)\nenet_scaled_mae","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enet_scaled2 = ElasticNet(alpha = start_model.alpha_,\n                        l1_ratio = start_model.l1_ratio_)\n\nenet_scaled_2, enet_scaled_2_mae = reg_pipe(enet_scaled2, RobustScaler(), X1_train, y1_train)\nenet_scaled_2_mae","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### **(d) XGBoost Regressor**\n\n*First Experiment: Without Standardisation*","metadata":{}},{"cell_type":"code","source":"xgb_reg = XGBRegressor(objective='reg:squarederror')\nxgbr, xgbr_mae = reg_train(X1_train, y1_train, xgb_reg)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T10:29:15.092057Z","iopub.execute_input":"2022-03-04T10:29:15.092641Z","iopub.status.idle":"2022-03-04T10:33:50.223878Z","shell.execute_reply.started":"2022-03-04T10:29:15.092605Z","shell.execute_reply":"2022-03-04T10:33:50.223194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Second Experiment: With Standardisation*","metadata":{}},{"cell_type":"code","source":"#MinMaxScaler\nxgbr1 = XGBRegressor(objective='reg:squarederror')\nxgbr_scaled, xgb1_scaled_mae = reg_pipe(xgbr1, MinMaxScaler(feature_range=(0,10)), X1_train, y1_train)\nxgb1_scaled_mae","metadata":{"execution":{"iopub.status.busy":"2022-03-04T10:33:50.227761Z","iopub.execute_input":"2022-03-04T10:33:50.22989Z","iopub.status.idle":"2022-03-04T10:44:04.199993Z","shell.execute_reply.started":"2022-03-04T10:33:50.229848Z","shell.execute_reply":"2022-03-04T10:44:04.199319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#RobustScaler\nxgbr2 = XGBRegressor(objective='reg:squarederror')\nxgbr2_scaled, xgb2_scaled_mae = reg_pipe(xgbr2, RobustScaler(), X1_train, y1_train)\nxgb2_scaled_mae","metadata":{"execution":{"iopub.status.busy":"2022-03-04T10:44:04.201675Z","iopub.execute_input":"2022-03-04T10:44:04.202125Z","iopub.status.idle":"2022-03-04T10:54:14.612325Z","shell.execute_reply.started":"2022-03-04T10:44:04.202093Z","shell.execute_reply":"2022-03-04T10:54:14.6117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<h4><b>6. Regression Leaderboard</b></h4>\nTo rank our models properly, we take the absolute value of the negative MAE scores since the innacuracy of predictions is based upon the magnitude of the MAE loss and not the direction.","metadata":{}},{"cell_type":"code","source":"unpiped_models_reg = [rforest_mae, lgb_mae, enet_mae, xgbr_mae]\npiped_models_reg = np.absolute([r_forest_scaled_mae, lgb_scaled_mae, enet_scaled_mae, xgb1_scaled_mae])\npiped_models_reg2 = np.absolute([r_forest_scaled_2_mae, lgb_scaled_2_mae, enet_scaled_2_mae, xgb2_scaled_mae])\n\nreg_model_index = ['Random Forest Regressor','LightGBM Regressor', 'ENet Regularised','XGBoost Regressor']\npre_leaderboard_2 = pd.DataFrame({'MAE (Without Standardisation)': unpiped_models_reg, \n                                  'MAE (With MinMaxScaler)': piped_models_reg,\n                                  'MAE (With RobustScaler)': piped_models_reg2}, index = reg_model_index)\n\npd.set_option('display.float_format', lambda x: '%.15f' % x)\npre_leaderboard_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre_leaderboard_2.nsmallest(4, 'MAE (Without Standardisation)')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre_leaderboard_2.nsmallest(4, 'MAE (With MinMaxScaler)')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre_leaderboard_2.nsmallest(4, 'MAE (With RobustScaler)')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the leaderboard, we see that the champion model for the regression task is the XGBoost Regressor which outperformed others across the three feature variations. We are now going to unlock our test (holdout) set for evaluation using XGBoost with RobustScaler.","metadata":{}},{"cell_type":"markdown","source":"  ","metadata":{}},{"cell_type":"markdown","source":"<h4><b>6. Holdout Evaluation</b></h4>","metadata":{}},{"cell_type":"markdown","source":"<b> (a) Top Performing Models </b>\n- From the classification experiments, the champion model was the **XGBoost Classifier with RobustScaler preprocessing**\n- From the regression experiments, the champion model was the **XGBoost Regressor with RobustScaler preprocessing**","metadata":{"execution":{"iopub.status.busy":"2022-03-03T19:17:37.557513Z","iopub.execute_input":"2022-03-03T19:17:37.557944Z","iopub.status.idle":"2022-03-03T19:17:37.565434Z","shell.execute_reply.started":"2022-03-03T19:17:37.557902Z","shell.execute_reply":"2022-03-03T19:17:37.564266Z"}}},{"cell_type":"markdown","source":"<b> (b) Evaluating Model Accuracy </b>\n\nNow, we unlock our holdout set and implement a pipeline to carry out the standardisation, prediction, and evaluation procedures. We will use the same evaluation metrics we used during cross-validation, namely: LogLoss Categorical Accuracy and Mean Absolute Error (MAE).","metadata":{}},{"cell_type":"code","source":"def class_pipe_score(estimator):\n    \n    '''\n    This function constructs a pipeline of standardisation and modelling \n    steps for our classification champion model and returns the specified accuracy score \n    from the evaluation of the test set.\n    '''\n    \n    pipe = Pipeline([('scaler', RobustScaler()), ('model', estimator)])\n    model = pipe.fit(X_train, y_train)\n    \n    prediction = model.predict_proba(X_test)\n    evaluation = log_loss(y_test, prediction, labels = [0,1,2])\n    \n    return model, evaluation\n\ndef reg_pipe_score(estimator):\n    \n    '''\n    This function constructs a pipeline of standardisation and modelling \n    steps for our regression champion model and returns the specified accuracy score \n    from the evaluation of the test set.\n    '''\n    \n    pipe = Pipeline([('scaler', RobustScaler()), ('model', estimator)])\n    model = pipe.fit(X1_train, y1_train)\n    \n    prediction = model.predict(X1_test)\n    evaluation = mean_absolute_error(y1_test, prediction)\n    \n    return model, evaluation","metadata":{"execution":{"iopub.status.busy":"2022-03-04T10:56:49.200907Z","iopub.execute_input":"2022-03-04T10:56:49.201213Z","iopub.status.idle":"2022-03-04T10:56:49.209708Z","shell.execute_reply.started":"2022-03-04T10:56:49.201172Z","shell.execute_reply":"2022-03-04T10:56:49.2086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_model, class_eval = class_pipe_score(xgb_scaled_2)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T10:56:50.143185Z","iopub.execute_input":"2022-03-04T10:56:50.143686Z","iopub.status.idle":"2022-03-04T10:57:27.274886Z","shell.execute_reply.started":"2022-03-04T10:56:50.143654Z","shell.execute_reply":"2022-03-04T10:57:27.273952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg_model, reg_eval = reg_pipe_score(xgbr2_scaled)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T10:57:27.276879Z","iopub.execute_input":"2022-03-04T10:57:27.277212Z","iopub.status.idle":"2022-03-04T10:57:42.512112Z","shell.execute_reply.started":"2022-03-04T10:57:27.277169Z","shell.execute_reply":"2022-03-04T10:57:42.511262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(class_eval)\nprint(reg_eval)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T10:57:42.513448Z","iopub.execute_input":"2022-03-04T10:57:42.513918Z","iopub.status.idle":"2022-03-04T10:57:42.519463Z","shell.execute_reply.started":"2022-03-04T10:57:42.51388Z","shell.execute_reply":"2022-03-04T10:57:42.518604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Here's a summary table of the two champion models we obtained and their accuracy scores.***","metadata":{}},{"cell_type":"code","source":"metrics = ['LogLoss','MAE']\nscores = [class_eval,reg_eval]\nindex = ['XGBoost Classifier','XGBoost Regressor']\n\npd.set_option('display.float_format', lambda x: '%.15f' % x)\nmodel_summary = pd.DataFrame({'Evaluation Metric':metrics, 'Evaluation Score':scores}, index = index)\nmodel_summary","metadata":{"execution":{"iopub.status.busy":"2022-03-04T10:59:13.631879Z","iopub.execute_input":"2022-03-04T10:59:13.632437Z","iopub.status.idle":"2022-03-04T10:59:13.64409Z","shell.execute_reply.started":"2022-03-04T10:59:13.632394Z","shell.execute_reply":"2022-03-04T10:59:13.643524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4> </h4>\n\n## Part IV: Predictions and Exporting","metadata":{}},{"cell_type":"code","source":"import pickle\nimport json","metadata":{"execution":{"iopub.status.busy":"2022-03-04T10:59:23.033149Z","iopub.execute_input":"2022-03-04T10:59:23.033797Z","iopub.status.idle":"2022-03-04T10:59:23.037399Z","shell.execute_reply.started":"2022-03-04T10:59:23.033759Z","shell.execute_reply":"2022-03-04T10:59:23.036756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def twin_predictor(input_data, estimators):\n    \"\"\"\n    reads data, processes it for the classification and regression tasks, \n    passes it into \"models\" (a stack of the classifier and regressor) for inference,\n    and returns a json response of predictions across the time intervals\n    of {5,10,15,30,60} minutes after a session timestamp.\n    \"\"\"\n    \n    #Split data into weather data and rain data to be taken in by the respective models\n    weather_input = input_data.drop('M_WEATHER', axis = 1)\n    rain_input = input_data.drop('M_RAIN_PERCENTAGE', axis = 1)\n    X_list = [weather_input, rain_input] \n\n    export = dict()\n    intervals = [5,10,15,30,60]\n    weather_type = ['Clear', 'Light Cloud', 'Overcast', 'Light Rain', 'Heavy Rain', 'Storm']\n    \n    for time in intervals:\n        \n        #Assign selected offset to the offset fields\n        weather_input['M_TIME_OFFSET'] = time\n        rain_input['M_TIME_OFFSET'] = time\n        \n        #Run inference\n        prediction1 = estimators[0].predict(X_list[0])\n        prediction2 = estimators[1].predict(X_list[1])\n        \n        #Output results\n        for i,j in enumerate(weather_type):\n            if int(prediction1) == i:\n                export['{} min'.format(time)] = {\n                    'Weather Type': '{} ({})'.format(i,j),\n                    'Rain Percentage' : round(prediction2[0],2).item()\n                }\n        \n    return json.dumps(export)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:16:46.011184Z","iopub.execute_input":"2022-03-04T11:16:46.011506Z","iopub.status.idle":"2022-03-04T11:16:46.02192Z","shell.execute_reply.started":"2022-03-04T11:16:46.011459Z","shell.execute_reply":"2022-03-04T11:16:46.021096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = [class_model, reg_model]\nfull_test_data = final_data_weather[(final_data_weather.drop('M_WEATHER', axis = 1).isin(X_test)).all(axis=1)]\ntest_data = full_test_data.iloc[2000:2001]","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:12:26.551195Z","iopub.execute_input":"2022-03-04T11:12:26.551666Z","iopub.status.idle":"2022-03-04T11:12:26.628424Z","shell.execute_reply.started":"2022-03-04T11:12:26.551626Z","shell.execute_reply":"2022-03-04T11:12:26.627491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"twin_predictor(test_data, models)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:16:21.428642Z","iopub.execute_input":"2022-03-04T11:16:21.428917Z","iopub.status.idle":"2022-03-04T11:16:21.493775Z","shell.execute_reply.started":"2022-03-04T11:16:21.428889Z","shell.execute_reply":"2022-03-04T11:16:21.492964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**(b) Exporting, loading and testing**","metadata":{}},{"cell_type":"code","source":"twin_ai_features = './twin_ai_features.pkl'\npickle.dump(final_data_weather.columns, open(twin_ai_features, 'wb'))\n\nestimators = [class_model, reg_model]\ntwin_ai_predictor = './estimators.pkl'\npickle.dump(estimators, open('twin_ai_predictor', 'wb'))","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:18:57.381727Z","iopub.execute_input":"2022-03-04T11:18:57.382663Z","iopub.status.idle":"2022-03-04T11:18:57.435945Z","shell.execute_reply.started":"2022-03-04T11:18:57.382607Z","shell.execute_reply":"2022-03-04T11:18:57.435307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"load_model = pickle.load(open('twin_ai_predictor', 'rb'))\nmake_prediction = twin_predictor(test_data, load_model)\n\nprint(make_prediction)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T11:20:01.804753Z","iopub.execute_input":"2022-03-04T11:20:01.805023Z","iopub.status.idle":"2022-03-04T11:20:01.928552Z","shell.execute_reply.started":"2022-03-04T11:20:01.804995Z","shell.execute_reply":"2022-03-04T11:20:01.927827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Next Steps\n\nHaving completed the machine learning workflow for this challenge, we will continue work on the following objectives to take our models out of the notebook and into the real world:\n- Deploy the models into a containerised service on Kubernetes via Oracle Cloud\n- Set up real-time inference pipeline for online predictions\n- Expose the inference API to be consumed by a frontend UI built with React.js and fastAPI.\n- Set up MLOps infrastructure and workflows to monitor model performance in production\n- Continuously retraining model to achieve better results and to ensure stability of model quality in production","metadata":{}},{"cell_type":"markdown","source":"## Closing Remarks/Acknowledgements\n\nWe sincerely thank Hackmakers and Oracle for hosting this amazing contest. It was a fun and challenging experience, albeit with a mix of severe external limitations we had to overcome. We look forward to seeing the winning solutions and how other approached the problem. We will continue to learn more and more about the Formula One ecosystem, and we hope to discover more of the amazing work that the RedBull Research team is doing.","metadata":{}},{"cell_type":"markdown","source":" #### **Notebook by Team Twin AI**","metadata":{}}]}